{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d731a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to test and train a ROCKET model on our data using stratified kfold cross-validation\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "CONTROL = 'TERBINAFINE- (control)'\n",
    "TREATED = 'TERBINAFINE+'\n",
    "PROCESSED_DIR = 'data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cac1794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    \"\"\" We load the data with padding to the longest time series to make them all the same length \"\"\"\n",
    "    X, y = [], []\n",
    "    for treatment in [CONTROL, TREATED]:\n",
    "        treatment_dir = os.path.join(data_dir, treatment)\n",
    "        for file_name in os.listdir(treatment_dir):\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(treatment_dir, file_name)\n",
    "                df = pd.read_csv(file_path)\n",
    "                time_series = df[['X', 'Y', 'Speed']].values\n",
    "                X.append(time_series)\n",
    "                y.append(treatment)\n",
    "    # Pad sequences to the same length\n",
    "    max_length = max(len(ts) for ts in X)\n",
    "    X_padded = []\n",
    "    for ts in X:\n",
    "        padding_length = max_length - len(ts)\n",
    "        if padding_length > 0:\n",
    "            padding = np.zeros((padding_length, ts.shape[1]))\n",
    "            ts_padded = np.vstack([ts, padding])\n",
    "        else:\n",
    "            ts_padded = ts\n",
    "        X_padded.append(ts_padded)\n",
    "    X_array = np.array(X_padded)\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    return X_array, y_encoded\n",
    "\n",
    "X, y = load_data(PROCESSED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee068ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose for ROCKET input shape (n_instances, n_channels, n_timepoints)\n",
    "X_transposed = X.transpose(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7f2488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PanelStandardScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        n_instances, n_channels, n_timepoints = X.shape\n",
    "        X_reshaped = X.reshape(n_instances, -1)\n",
    "        self.scaler.fit(X_reshaped)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        n_instances, n_channels, n_timepoints = X.shape\n",
    "        X_reshaped = X.reshape(n_instances, -1)\n",
    "        X_scaled = self.scaler.transform(X_reshaped)\n",
    "        return X_scaled.reshape(n_instances, n_channels, n_timepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aff02996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/skbase/base/_base.py:1342: FutureWarning: tag 'handles-missing-data' will be removed in sktime version 1.0.0 and replaced by 'capability:missing_values', please use 'capability:missing_values' instead\n",
      "  self._deprecate_tag_warn(collected_tags)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/skbase/base/_base.py:1342: FutureWarning: tag 'handles-missing-data' will be removed in sktime version 1.0.0 and replaced by 'capability:missing_values', please use 'capability:missing_values' instead\n",
      "  self._deprecate_tag_warn(collected_tags)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/skbase/base/_base.py:1342: FutureWarning: tag 'handles-missing-data' will be removed in sktime version 1.0.0 and replaced by 'capability:missing_values', please use 'capability:missing_values' instead\n",
      "  self._deprecate_tag_warn(collected_tags)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/skbase/base/_base.py:1342: FutureWarning: tag 'handles-missing-data' will be removed in sktime version 1.0.0 and replaced by 'capability:missing_values', please use 'capability:missing_values' instead\n",
      "  self._deprecate_tag_warn(collected_tags)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/skbase/base/_base.py:1342: FutureWarning: tag 'handles-missing-data' will be removed in sktime version 1.0.0 and replaced by 'capability:missing_values', please use 'capability:missing_values' instead\n",
      "  self._deprecate_tag_warn(collected_tags)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "TERBINAFINE- (control)       0.68      0.81      0.74        26\n",
      "          TERBINAFINE+       0.76      0.62      0.68        26\n",
      "\n",
      "              accuracy                           0.71        52\n",
      "             macro avg       0.72      0.71      0.71        52\n",
      "          weighted avg       0.72      0.71      0.71        52\n",
      "\n",
      "[[21  5]\n",
      " [10 16]]\n",
      "----------------------------------------\n",
      "Fold 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/skbase/base/_base.py:1342: FutureWarning: tag 'handles-missing-data' will be removed in sktime version 1.0.0 and replaced by 'capability:missing_values', please use 'capability:missing_values' instead\n",
      "  self._deprecate_tag_warn(collected_tags)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/skbase/base/_base.py:1342: FutureWarning: tag 'handles-missing-data' will be removed in sktime version 1.0.0 and replaced by 'capability:missing_values', please use 'capability:missing_values' instead\n",
      "  self._deprecate_tag_warn(collected_tags)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/skbase/base/_base.py:1342: FutureWarning: tag 'handles-missing-data' will be removed in sktime version 1.0.0 and replaced by 'capability:missing_values', please use 'capability:missing_values' instead\n",
      "  self._deprecate_tag_warn(collected_tags)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/skbase/base/_base.py:1342: FutureWarning: tag 'handles-missing-data' will be removed in sktime version 1.0.0 and replaced by 'capability:missing_values', please use 'capability:missing_values' instead\n",
      "  self._deprecate_tag_warn(collected_tags)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/skbase/base/_base.py:1342: FutureWarning: tag 'handles-missing-data' will be removed in sktime version 1.0.0 and replaced by 'capability:missing_values', please use 'capability:missing_values' instead\n",
      "  self._deprecate_tag_warn(collected_tags)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "TERBINAFINE- (control)       0.71      0.85      0.77        26\n",
      "          TERBINAFINE+       0.81      0.65      0.72        26\n",
      "\n",
      "              accuracy                           0.75        52\n",
      "             macro avg       0.76      0.75      0.75        52\n",
      "          weighted avg       0.76      0.75      0.75        52\n",
      "\n",
      "[[22  4]\n",
      " [ 9 17]]\n",
      "----------------------------------------\n",
      "\n",
      "Average metrics over all folds:\n",
      "Accuracy: 0.7308\n",
      "Precision: 0.7396\n",
      "Recall: 0.7308\n",
      "F1-score: 0.7283\n"
     ]
    }
   ],
   "source": [
    "USE_SCALER = True  # Set to False to disable StandardScaler\n",
    "\n",
    "n_splits = 2\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "accuracies, precisions, recalls, f1s = [], [], [], []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X_transposed, y)):\n",
    "    print(f\"Fold {fold+1}/{n_splits}\")\n",
    "    X_train, X_test = X_transposed[train_idx], X_transposed[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    classifier = RocketClassifier(\n",
    "        num_kernels=10000, \n",
    "        random_state=42,\n",
    "        rocket_transform=\"minirocket\" \n",
    "    )\n",
    "    if USE_SCALER:\n",
    "        pipeline = make_pipeline(PanelStandardScaler(), classifier)\n",
    "    else:\n",
    "        pipeline = make_pipeline(classifier)\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted')\n",
    "    rec = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "    f1s.append(f1)\n",
    "\n",
    "    print(classification_report(y_test, y_pred, target_names=[CONTROL, TREATED]))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\"*40)\n",
    "\n",
    "print(\"\\nAverage metrics over all folds:\")\n",
    "print(f\"Accuracy: {np.mean(accuracies):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f}\")\n",
    "print(f\"Recall: {np.mean(recalls):.4f}\")\n",
    "print(f\"F1-score: {np.mean(f1s):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "781e7bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pli 1/5 ---\n",
      "Précision (Seuil 0.4): 0.6667\n",
      "\n",
      "--- Pli 2/5 ---\n",
      "Précision (Seuil 0.4): 0.6667\n",
      "\n",
      "--- Pli 2/5 ---\n",
      "Précision (Seuil 0.4): 0.8571\n",
      "\n",
      "--- Pli 3/5 ---\n",
      "Précision (Seuil 0.4): 0.8571\n",
      "\n",
      "--- Pli 3/5 ---\n",
      "Précision (Seuil 0.4): 0.7619\n",
      "\n",
      "--- Pli 4/5 ---\n",
      "Précision (Seuil 0.4): 0.7619\n",
      "\n",
      "--- Pli 4/5 ---\n",
      "Précision (Seuil 0.4): 0.8571\n",
      "\n",
      "--- Pli 5/5 ---\n",
      "Précision (Seuil 0.4): 0.8571\n",
      "\n",
      "--- Pli 5/5 ---\n",
      "Précision (Seuil 0.4): 0.7500\n",
      "\n",
      "==================================================\n",
      "RÉSULTATS CUMULÉS (Seuil ajusté à 0.4)\n",
      "==================================================\n",
      "\n",
      "--- Rapport de Classification Final ---\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "TERBINAFINE- (control)       0.87      0.65      0.75        52\n",
      "          TERBINAFINE+       0.72      0.90      0.80        52\n",
      "\n",
      "              accuracy                           0.78       104\n",
      "             macro avg       0.80      0.78      0.78       104\n",
      "          weighted avg       0.80      0.78      0.78       104\n",
      "\n",
      "\n",
      "--- Matrice de Confusion Finale ---\n",
      "[[34 18]\n",
      " [ 5 47]]\n",
      "Précision (Seuil 0.4): 0.7500\n",
      "\n",
      "==================================================\n",
      "RÉSULTATS CUMULÉS (Seuil ajusté à 0.4)\n",
      "==================================================\n",
      "\n",
      "--- Rapport de Classification Final ---\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "TERBINAFINE- (control)       0.87      0.65      0.75        52\n",
      "          TERBINAFINE+       0.72      0.90      0.80        52\n",
      "\n",
      "              accuracy                           0.78       104\n",
      "             macro avg       0.80      0.78      0.78       104\n",
      "          weighted avg       0.80      0.78      0.78       104\n",
      "\n",
      "\n",
      "--- Matrice de Confusion Finale ---\n",
      "[[34 18]\n",
      " [ 5 47]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# Nouveaux imports pour la pipeline ROCKET en deux étapes et le classifieur\n",
    "from sktime.transformations.panel.rocket import MiniRocketMultivariate \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "CONTROL = 'TERBINAFINE- (control)'\n",
    "TREATED = 'TERBINAFINE+'\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "\n",
    "# --- 1. CLASSE DE PADDING ET CHARGEMENT (Identique, nécessaire) ---\n",
    "def load_data(data_dir):\n",
    "    \"\"\" We load the data with padding to the longest time series to make them all the same length \"\"\"\n",
    "    X, y = [], []\n",
    "    for treatment in [CONTROL, TREATED]:\n",
    "        treatment_dir = os.path.join(data_dir, treatment)\n",
    "        # Vérifiez l'existence du répertoire ici pour éviter l'erreur\n",
    "        if not os.path.exists(treatment_dir):\n",
    "            print(f\"ATTENTION: Le répertoire {treatment_dir} n'existe pas. Veuillez vérifier le chemin.\")\n",
    "            continue\n",
    "            \n",
    "        for file_name in os.listdir(treatment_dir):\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(treatment_dir, file_name)\n",
    "                df = pd.read_csv(file_path)\n",
    "                time_series = df[['X', 'Y', 'Speed']].values\n",
    "                X.append(time_series)\n",
    "                y.append(treatment)\n",
    "    \n",
    "    if not X:\n",
    "        print(\"Aucune donnée chargée.\")\n",
    "        return np.array([]), np.array([])\n",
    "        \n",
    "    # Pad sequences to the same length\n",
    "    max_length = max(len(ts) for ts in X)\n",
    "    X_padded = []\n",
    "    for ts in X:\n",
    "        padding_length = max_length - len(ts)\n",
    "        if padding_length > 0:\n",
    "            padding = np.zeros((padding_length, ts.shape[1]), dtype=ts.dtype)\n",
    "            ts_padded = np.vstack([ts, padding])\n",
    "        else:\n",
    "            ts_padded = ts\n",
    "        X_padded.append(ts_padded)\n",
    "        \n",
    "    X_array = np.array(X_padded)\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    return X_array, y_encoded\n",
    "\n",
    "# --- 2. CLASSE DE NORMALISATION (Utilisée si besoin) ---\n",
    "class PanelStandardScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        n_instances, n_channels, n_timepoints = X.shape\n",
    "        # Aplatit pour ajuster un seul scaler sur toutes les frames/canaux\n",
    "        X_reshaped = X.reshape(n_instances * n_channels, n_timepoints).T\n",
    "        self.scaler.fit(X_reshaped)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        n_instances, n_channels, n_timepoints = X.shape\n",
    "        X_reshaped = X.reshape(n_instances * n_channels, n_timepoints).T\n",
    "        X_scaled = self.scaler.transform(X_reshaped)\n",
    "        # Réorganise au format (n_instances, n_channels, n_timepoints)\n",
    "        return X_scaled.T.reshape(n_instances, n_channels, n_timepoints)\n",
    "\n",
    "# --- 3. CHARGEMENT ET PRÉPARATION GLOBALE ---\n",
    "X_initial, y = load_data(PROCESSED_DIR)\n",
    "# Transpose for ROCKET input shape (n_instances, n_channels, n_timepoints)\n",
    "X = X_initial.transpose(0, 2, 1)\n",
    "\n",
    "# --- 4. PARAMÈTRES ET HYPERPARAMÈTRES ---\n",
    "USE_SCALER = False  # Maintenu à False si la normalisation détruit le signal\n",
    "THRESHOLD = 0.40    # NOUVEAU SEUIL pour favoriser le Rappel (Descendu de 0.50 à 0.40)\n",
    "N_SPLITS = 5        # Augmenté à 5 pour une meilleure validation\n",
    "NUM_KERNELS = 1000 # Augmenté pour de meilleures performances (si le temps le permet)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# --- 5. BOUGLE D'ENTRAÎNEMENT ET D'ÉVALUATION ---\n",
    "\n",
    "# Liste pour collecter toutes les prédictions finales (pour l'analyse globale)\n",
    "y_true_total, y_pred_total_tuned = np.array([]), np.array([])\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"\\n--- Pli {fold+1}/{N_SPLITS} ---\")\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # --- DEFINITION DU MODELE ---\n",
    "    rocket_transformer = MiniRocketMultivariate(\n",
    "        num_kernels=NUM_KERNELS, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # NOUVEAU CLASSIFIEUR : Régression Logistique avec class_weight='balanced'\n",
    "    # 'balanced' donne plus d'importance à la classe la moins bien prédite (TERBINAFINE+)\n",
    "    classifier = LogisticRegression(\n",
    "        solver='liblinear', \n",
    "        class_weight='balanced', \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Construction de la pipeline\n",
    "    steps = []\n",
    "    if USE_SCALER:\n",
    "        steps.append(PanelStandardScaler())\n",
    "    steps.extend([rocket_transformer, classifier])\n",
    "    \n",
    "    pipeline = make_pipeline(*steps)\n",
    "\n",
    "    # Entraînement\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # --- PRÉDICTION et AJUSTEMENT du SEUIL (Threshold Tuning) ---\n",
    "    \n",
    "    # 1. Obtenir les probabilités de la classe 1 (TERBINAFINE+)\n",
    "    y_proba = pipeline.predict_proba(X_test)[:, 1] \n",
    "    \n",
    "    # 2. Appliquer le seuil ajusté (favorise le rappel)\n",
    "    y_pred_tuned = (y_proba > THRESHOLD).astype(int)\n",
    "\n",
    "    # --- Évaluation du Pli ---\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred_tuned)\n",
    "    prec = precision_score(y_test, y_pred_tuned, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_tuned, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_tuned, average='weighted', zero_division=0)\n",
    "    \n",
    "    print(f\"Précision (Seuil {THRESHOLD}): {acc:.4f}\")\n",
    "    \n",
    "    # Collection des résultats pour le rapport final cumulé\n",
    "    y_true_total = np.concatenate([y_true_total, y_test])\n",
    "    y_pred_total_tuned = np.concatenate([y_pred_total_tuned, y_pred_tuned])\n",
    "\n",
    "# --- 6. RÉSULTATS CUMULÉS ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"RÉSULTATS CUMULÉS (Seuil ajusté à {THRESHOLD})\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n--- Rapport de Classification Final ---\")\n",
    "print(classification_report(y_true_total, y_pred_total_tuned, target_names=[CONTROL, TREATED], zero_division=0))\n",
    "print(\"\\n--- Matrice de Confusion Finale ---\")\n",
    "print(confusion_matrix(y_true_total, y_pred_total_tuned))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
