{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da3bb91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 54000\n",
      "Number of segments: 60\n",
      "\n",
      "Segment sizes:\n",
      "segment\n",
      "0     901\n",
      "1     900\n",
      "2     900\n",
      "3     900\n",
      "4     900\n",
      "5     900\n",
      "6     900\n",
      "7     900\n",
      "8     900\n",
      "9     900\n",
      "10    900\n",
      "11    900\n",
      "12    900\n",
      "13    900\n",
      "14    900\n",
      "15    900\n",
      "16    900\n",
      "17    900\n",
      "18    900\n",
      "19    900\n",
      "20    900\n",
      "21    900\n",
      "22    900\n",
      "23    900\n",
      "24    900\n",
      "25    900\n",
      "26    900\n",
      "27    900\n",
      "28    900\n",
      "29    900\n",
      "30    900\n",
      "31    900\n",
      "32    900\n",
      "33    900\n",
      "34    900\n",
      "35    900\n",
      "36    900\n",
      "37    900\n",
      "38    900\n",
      "39    900\n",
      "40    900\n",
      "41    900\n",
      "42    900\n",
      "43    900\n",
      "44    900\n",
      "45    900\n",
      "46    900\n",
      "47    900\n",
      "48    900\n",
      "49    900\n",
      "50    900\n",
      "51    900\n",
      "52    900\n",
      "53    900\n",
      "54    900\n",
      "55    900\n",
      "56    900\n",
      "57    900\n",
      "58    900\n",
      "59    899\n",
      "dtype: int64\n",
      "\n",
      "First few rows:\n",
      "       GlobalFrame                  Timestamp     Speed      Fragment  \\\n",
      "53990        53991 2024-09-11 10:12:40.003827  0.159871  fragment_4_3   \n",
      "53991        53992 2024-09-11 10:12:42.003887  0.043081  fragment_4_3   \n",
      "53992        53993 2024-09-11 10:12:44.004427  0.345708  fragment_4_3   \n",
      "53993        53994 2024-09-11 10:12:46.003809  0.300798  fragment_4_3   \n",
      "53994        53995 2024-09-11 10:12:48.003904  0.175176  fragment_4_3   \n",
      "53995        53996 2024-09-11 10:12:50.004370  0.000000  fragment_4_3   \n",
      "53996        53997 2024-09-11 10:12:52.003827  0.000000  fragment_4_3   \n",
      "53997        53998 2024-09-11 10:12:54.003900  0.000000  fragment_4_3   \n",
      "53998        53999 2024-09-11 10:12:56.004303  0.000000  fragment_4_3   \n",
      "53999        54000 2024-09-11 10:12:58.003839  0.185518  fragment_4_3   \n",
      "\n",
      "       LocalFrame           X           Y  segment  \n",
      "53990       10790  428.932000  344.764000       59  \n",
      "53991       10791  428.952000  344.772000       59  \n",
      "53992       10792  429.003968  344.607143       59  \n",
      "53993       10793  429.070866  344.472441       59  \n",
      "53994       10794  429.105882  344.392157       59  \n",
      "53995       10795  429.105882  344.392157       59  \n",
      "53996       10796  429.105882  344.392157       59  \n",
      "53997       10797  429.105882  344.392157       59  \n",
      "53998       10798  429.105882  344.392157       59  \n",
      "53999       10799  429.043307  344.460630       59  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def segment_by_time_gap(coordinates, timestamps, gap_hours=2):\n",
    "    \"\"\"\n",
    "    Segment coordinates based on time gaps greater than specified hours.\n",
    "    \n",
    "    Parameters:\n",
    "    - coordinates: list of tuples (X, Y) or array of coordinates\n",
    "    - timestamps: list of datetime objects or timestamps\n",
    "    - gap_hours: time gap threshold in hours (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "    - List of segments, where each segment is a list of (coordinate, timestamp) tuples\n",
    "    \"\"\"\n",
    "    if len(coordinates) == 0 or len(timestamps) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Convert timestamps to pandas datetime if not already\n",
    "    timestamps = pd.to_datetime(timestamps)\n",
    "    \n",
    "    # Calculate time differences between consecutive points\n",
    "    time_diffs = timestamps[1:] - timestamps[:-1]\n",
    "    \n",
    "    # Find indices where gap is greater than threshold\n",
    "    gap_threshold = pd.Timedelta(hours=gap_hours)\n",
    "    split_indices = [0]  # Start with first index\n",
    "    \n",
    "    for i, diff in enumerate(time_diffs):\n",
    "        if diff > gap_threshold:\n",
    "            split_indices.append(i + 1)\n",
    "    \n",
    "    split_indices.append(len(coordinates))  # Add end index\n",
    "    \n",
    "    # Create segments\n",
    "    segments = []\n",
    "    for i in range(len(split_indices) - 1):\n",
    "        start_idx = split_indices[i]\n",
    "        end_idx = split_indices[i + 1]\n",
    "        segment = list(zip(coordinates[start_idx:end_idx], timestamps[start_idx:end_idx]))\n",
    "        segments.append(segment)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "\n",
    "def add_segment_column_to_csv(file_path, gap_hours=2):\n",
    "    \"\"\"\n",
    "    Add a 'segment' column to CSV data based on time gaps.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: path to the CSV file\n",
    "    - gap_hours: time gap threshold in hours (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with added 'segment' column\n",
    "    \"\"\"\n",
    "    # Read CSV - first row has headers\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert Timestamp to datetime\n",
    "    data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "    \n",
    "    # Segment by time gap\n",
    "    coordinates = list(zip(data['X'], data['Y']))\n",
    "    timestamps = data['Timestamp'].tolist()\n",
    "    \n",
    "    segments = segment_by_time_gap(coordinates, timestamps, gap_hours)\n",
    "    \n",
    "    # Create a segment column\n",
    "    segment_col = []\n",
    "    for seg_idx, segment in enumerate(segments):\n",
    "        segment_col.extend([seg_idx] * len(segment))\n",
    "    \n",
    "    data['segment'] = segment_col\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Example usage with the file from context\n",
    "example_file = 'data/TERBINAFINE+/coordinates_highestspeed_20240827_11_3_with_time_speed.csv'\n",
    "segmented_data = add_segment_column_to_csv(example_file)\n",
    "\n",
    "print(f\"Total rows: {len(segmented_data)}\")\n",
    "print(f\"Number of segments: {segmented_data['segment'].nunique()}\")\n",
    "print(f\"\\nSegment sizes:\")\n",
    "print(segmented_data.groupby('segment').size())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(segmented_data.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b360e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3be8b6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 52 files from TERBINAFINE- (control)...\n",
      "  Processed 10/10 files...\n",
      "  Processed 10/10 files...\n",
      "  Processed 20/20 files...\n",
      "  Processed 20/20 files...\n",
      "  Processed 30/30 files...\n",
      "  Processed 30/30 files...\n",
      "  Processed 40/40 files...\n",
      "  Processed 40/40 files...\n",
      "  Processed 50/50 files...\n",
      "  Processed 50/50 files...\n",
      "\n",
      "Processing 52 files from TERBINAFINE+...\n",
      "\n",
      "Processing 52 files from TERBINAFINE+...\n",
      "  Processed 60/60 files...\n",
      "  Processed 60/60 files...\n",
      "  Processed 70/70 files...\n",
      "  Processed 70/70 files...\n",
      "  Processed 80/80 files...\n",
      "  Processed 80/80 files...\n",
      "  Processed 90/90 files...\n",
      "  Processed 90/90 files...\n",
      "  Processed 100/100 files...\n",
      "  Processed 100/100 files...\n",
      "\n",
      "============================================================\n",
      "Processing complete!\n",
      "Total files: 104\n",
      "Successfully processed: 104\n",
      "Failed: 0\n",
      "  TERBINAFINE- (control): 52 files\n",
      "  TERBINAFINE+: 52 files\n",
      "\n",
      "============================================================\n",
      "Processing complete!\n",
      "Total files: 104\n",
      "Successfully processed: 104\n",
      "Failed: 0\n",
      "  TERBINAFINE- (control): 52 files\n",
      "  TERBINAFINE+: 52 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def process_all_coordinate_files(input_dir='data', output_dir='data/partly_processed', gap_hours=2):\n",
    "    \"\"\"\n",
    "    Process all coordinate CSV files by adding segment information.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_dir: root directory containing TERBINAFINE folders (default: 'data')\n",
    "    - output_dir: directory to save processed files (default: 'data/partly_processed')\n",
    "    - gap_hours: time gap threshold in hours for segmentation (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with processing statistics\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process both TERBINAFINE- and TERBINAFINE+ folders\n",
    "    folders = ['TERBINAFINE- (control)', 'TERBINAFINE+']\n",
    "    stats = {\n",
    "        'total_files': 0,\n",
    "        'processed_files': 0,\n",
    "        'failed_files': [],\n",
    "        'files_by_folder': {}\n",
    "    }\n",
    "    \n",
    "    for folder in folders:\n",
    "        folder_path = Path(input_dir) / folder\n",
    "        \n",
    "        if not folder_path.exists():\n",
    "            print(f\"Folder not found: {folder_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Create corresponding output subfolder\n",
    "        output_subfolder = output_path / folder\n",
    "        output_subfolder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Find all coordinate CSV files\n",
    "        csv_files = list(folder_path.glob('coordinates_*.csv'))\n",
    "        stats['files_by_folder'][folder] = len(csv_files)\n",
    "        \n",
    "        print(f\"\\nProcessing {len(csv_files)} files from {folder}...\")\n",
    "        \n",
    "        for csv_file in csv_files:\n",
    "            stats['total_files'] += 1\n",
    "            \n",
    "            try:\n",
    "                # Process the file\n",
    "                segmented_data = add_segment_column_to_csv(str(csv_file), gap_hours)\n",
    "                \n",
    "                # Save to output directory with same filename\n",
    "                output_file = output_subfolder / csv_file.name\n",
    "                segmented_data.to_csv(output_file, index=False)\n",
    "                \n",
    "                stats['processed_files'] += 1\n",
    "                \n",
    "                # Print progress every 10 files\n",
    "                if stats['processed_files'] % 10 == 0:\n",
    "                    print(f\"  Processed {stats['processed_files']}/{stats['total_files']} files...\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {csv_file.name}: {str(e)}\")\n",
    "                stats['failed_files'].append(str(csv_file))\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing complete!\")\n",
    "    print(f\"Total files: {stats['total_files']}\")\n",
    "    print(f\"Successfully processed: {stats['processed_files']}\")\n",
    "    print(f\"Failed: {len(stats['failed_files'])}\")\n",
    "    \n",
    "    for folder, count in stats['files_by_folder'].items():\n",
    "        print(f\"  {folder}: {count} files\")\n",
    "    \n",
    "    if stats['failed_files']:\n",
    "        print(f\"\\nFailed files:\")\n",
    "        for f in stats['failed_files']:\n",
    "            print(f\"  - {f}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "# Run the processing\n",
    "processing_stats = process_all_coordinate_files(gap_hours=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea43fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now rename the files by their wormid directly and save them all in the same processed folder\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "input_dir = 'data/partly_processed'\n",
    "output_dir = 'data/processed'\n",
    "\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "processed_count = 0\n",
    "for folder in ['TERBINAFINE- (control)', 'TERBINAFINE+']:\n",
    "    folder_path = Path(input_dir) / folder\n",
    "    csv_files = list(folder_path.glob('coordinates_*.csv'))\n",
    "    \n",
    "    print(f\"Processing {len(csv_files)} files from {folder}...\")\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        # Extract wormid from filename\n",
    "        # Format: coordinates_highestspeed_YYYYMMDD_XX_Y_with_time_speed.csv\n",
    "        # We want: YYYYMMDD_XX_Y\n",
    "        filename_parts = csv_file.stem.split('_')  # Use stem to remove .csv\n",
    "        # Parts: ['coordinates', 'highestspeed', 'YYYYMMDD', 'XX', 'Y', 'with', 'time', 'speed']\n",
    "        date = filename_parts[2]\n",
    "        hour = filename_parts[3]\n",
    "        worm_num = filename_parts[4]\n",
    "        wormid = f\"{date}_{hour}_{worm_num}\"\n",
    "        \n",
    "        # Define new output file path\n",
    "        output_file = Path(output_dir) / f'{wormid}.csv'\n",
    "        \n",
    "        # Copy the file to the new location with the new name\n",
    "        segmented_data = pd.read_csv(csv_file)\n",
    "        segmented_data.to_csv(output_file, index=False)\n",
    "        processed_count += 1\n",
    "\n",
    "print(f\"\\nProcessing complete! Renamed and saved {processed_count} files to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f551fa83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
