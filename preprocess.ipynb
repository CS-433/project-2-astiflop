{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2dc2436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "CONTROL = 'TERBINAFINE- (control)'\n",
    "TREATED = 'TERBINAFINE+'\n",
    "PREPROCESSED_DIR = 'data/preprocessed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new files and Rename files according to their worm ID. For example, coordinates_highestspeed_20231010_12_01_...csv should be renamed to 20231010_piworm12_1.csv\n",
    "# Save the renamed files in PREPROCESSED_DIR/CONTROL and PREPROCESSED_DIR/TREATED\n",
    "\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    os.makedirs(treatment_dir, exist_ok=True)\n",
    "    for file_name in os.listdir('data/'+treatment):\n",
    "        if file_name.endswith('.csv'):\n",
    "            parts = file_name.split('_')\n",
    "            if len(str(parts[3])) == 1:\n",
    "                parts[3] = '0' + str(parts[3])\n",
    "            worm_id = f\"{parts[2]}_piworm{parts[3]}_{parts[4]}\"\n",
    "            df = pd.read_csv(os.path.join('data', treatment, file_name))\n",
    "            df.to_csv(os.path.join(treatment_dir, f\"{worm_id}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66acb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File counts match between original and processed directories.\n"
     ]
    }
   ],
   "source": [
    "# Check that we have the same number of files in PREPROCESSED_DIR/CONTROL and PREPROCESSED_DIR/TREATED as in the original directories\n",
    "original_control_files = [f for f in os.listdir('data/'+CONTROL) if f.endswith('.csv')]\n",
    "original_treated_files = [f for f in os.listdir('data/'+TREATED) if f.endswith('.csv')]\n",
    "processed_control_files = [f for f in os.listdir(os.path.join(PREPROCESSED_DIR, CONTROL)) if f.endswith('.csv')]\n",
    "processed_treated_files = [f for f in os.listdir(os.path.join(PREPROCESSED_DIR, TREATED)) if f.endswith('.csv')]\n",
    "assert len(original_control_files) == len(processed_control_files), \"Mismatch in CONTROL files count\"\n",
    "assert len(original_treated_files) == len(processed_treated_files), \"Mismatch in TREATED files count\"\n",
    "print(\"File counts match between original and processed directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11614226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first row if it contains NaN in 'Timestamp' column or if the the absolute difference between the first two timestamps is larger than 1 hours\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    for file_name in os.listdir(treatment_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(treatment_dir, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "            if pd.isna(df.loc[0, 'Timestamp']) or abs((df.loc[1, 'Timestamp'] - df.loc[0, 'Timestamp']).total_seconds()) > 3600:\n",
    "                df = df.drop(index=0).reset_index(drop=True)\n",
    "                df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c1142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files failing sanity check: 0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check the timestamp of the beginning of our dfs\n",
    "def sanity_check_first_timestamps(file):\n",
    "    df = pd.read_csv(file)\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    if len(df) >= 2:\n",
    "        if df.loc[0, 'Timestamp'] is pd.NaT:    \n",
    "            time_diff = abs((df.loc[2, 'Timestamp'] - df.loc[1, 'Timestamp']).total_seconds())\n",
    "        else:\n",
    "            time_diff = abs((df.loc[1, 'Timestamp'] - df.loc[0, 'Timestamp']).total_seconds())\n",
    "        if time_diff > 3600:  # 1 hour in seconds\n",
    "            return False\n",
    "        elif time_diff <= 3600:\n",
    "            return True\n",
    "        elif time_diff is pd.NaT:\n",
    "            return False\n",
    "        print(time_diff)\n",
    "    return False\n",
    "\n",
    "insanity_counts = 0\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    for file_name in os.listdir(treatment_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(treatment_dir, file_name)\n",
    "            if not sanity_check_first_timestamps(file_path):\n",
    "                print(f\"Sanity check failed for {file_name}\")\n",
    "                insanity_counts += 1\n",
    "print(f\"Total files failing sanity check: {insanity_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd883acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing timestamps found in TERBINAFINE- (control)/20250205_piworm10_2.csv\n",
      "Missing timestamps found in TERBINAFINE- (control)/20240924_piworm12_5.csv\n",
      "Missing timestamps found in TERBINAFINE+/20250205_piworm09_5.csv\n",
      "Missing timestamps found in TERBINAFINE+/20250205_piworm11_5.csv\n",
      "Total files with missing timestamps: 4\n",
      "Files with missing timestamps: ['data/processed/TERBINAFINE- (control)/20250205_piworm10_2.csv', 'data/processed/TERBINAFINE- (control)/20240924_piworm12_5.csv', 'data/processed/TERBINAFINE+/20250205_piworm09_5.csv', 'data/processed/TERBINAFINE+/20250205_piworm11_5.csv']\n"
     ]
    }
   ],
   "source": [
    "# See in which file there are missing timestamps\n",
    "count_missing_timestamps = 0\n",
    "file_with_missing_timestamps = []\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    for file_name in os.listdir(treatment_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(treatment_dir, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "            if df['Timestamp'].isnull().any():\n",
    "                print(f\"Missing timestamps found in {treatment}/{file_name}\")\n",
    "                count_missing_timestamps += 1\n",
    "                file_with_missing_timestamps.append(treatment_dir+'/'+file_name)\n",
    "                continue\n",
    "print(f\"Total files with missing timestamps: {count_missing_timestamps}\")\n",
    "print(f\"Files with missing timestamps: {file_with_missing_timestamps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c023e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Segment column to all files based on fixed frame counts.\n"
     ]
    }
   ],
   "source": [
    "# Add Segment column to our DF. A segment is composed of exactly 900 frames. Hence the first 900 frames are segment 0, the next 900 frames are segment 1, and so on.\n",
    "\n",
    "def add_segment_column_fixed_frames(file, frames_per_segment=900):\n",
    "    df = pd.read_csv(file)\n",
    "    num_rows = len(df)\n",
    "    df['Segment'] = np.arange(num_rows) // frames_per_segment\n",
    "    df.to_csv(file, index=False)\n",
    "\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    for file_name in os.listdir(treatment_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(treatment_dir, file_name)\n",
    "            add_segment_column_fixed_frames(file_path, frames_per_segment=900)\n",
    "\n",
    "print(\"Added Segment column to all files based on fixed frame counts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50ae6b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consecutive gap lengths in 'Speed' column for each file:\n",
      "20250311_piworm18_4.csv: [9, 17, 6, 3, 94, 11, 21, 3, 3, 46, 33, 22, 2, 77, 25, 51, 92, 14, 304, 33, 66, 3, 53, 506, 900, 900, 900, 286, 2, 37]\n",
      "20250311_piworm18_5.csv: [4, 37, 135, 53, 115, 40, 14, 115, 552, 370, 487, 315, 139, 171, 66, 506, 900, 900, 900, 505, 204, 43, 504, 900, 900]\n",
      "20250311_piworm18_6.csv: [15, 81, 10, 3, 13, 39, 131, 131, 278, 168, 58, 506, 395, 30, 506, 900, 197, 471, 765, 23, 113, 506, 395, 506, 231, 505, 395, 506, 900, 900, 900]\n",
      "20250311_piworm18_2.csv: [521, 1, 45, 65, 26, 8, 25, 206, 13, 506, 395, 169, 506, 395, 506, 900, 900, 900]\n",
      "20250311_piworm18_1.csv: [609, 535, 1, 110, 148, 269, 2, 2, 83, 129, 17, 150, 9, 2, 31, 233, 506, 900, 900, 900, 503, 397]\n",
      "20250415_piworm10_1.csv: [480, 393, 9, 85, 121, 6, 6, 5, 3, 6]\n",
      "20250415_piworm12_3.csv: [14, 41, 9, 35, 36, 9, 23, 1, 43, 33, 15, 16, 12, 12, 4, 12, 10, 3, 8, 3, 107, 9, 8, 23, 164, 47, 66, 32, 18, 2, 833, 900, 1]\n",
      "20250415_piworm12_2.csv: [143, 108, 403, 41, 376, 292, 110, 4, 4, 13, 7, 98, 211, 257, 17, 56, 2, 17, 21, 110, 195, 142, 113, 283, 739, 858, 44, 12, 350, 900, 900, 900, 900, 900, 900, 899]\n",
      "20250311_piworm19_6.csv: [35, 52, 80, 15, 44, 11, 21, 33, 21, 46, 7, 10, 60, 5, 11, 94, 81, 25, 188, 551, 900, 900, 360, 551, 900, 900, 899]\n",
      "20250311_piworm19_4.csv: [147, 31, 54, 14, 40, 50, 4, 4, 8, 24, 1, 30, 705, 1, 551, 900, 900, 900, 2, 2]\n",
      "20250415_piworm10_2.csv: [10, 1, 66, 27, 383, 116, 427, 6, 40, 617, 224, 41, 7, 656, 243, 658, 462, 658, 900, 900]\n",
      "20250415_piworm12_1.csv: [203, 14, 12, 33, 50, 47, 31, 49, 900, 1, 900, 1, 35, 79, 1, 45]\n",
      "20250415_piworm10_3.csv: [63, 30, 121, 27, 16, 29, 202, 47, 6, 329]\n",
      "20250311_piworm19_5.csv: [141, 170, 408, 893, 1, 2, 81, 166, 1, 3, 2, 4, 58, 3, 131, 1, 15, 109, 21, 9, 114, 551, 900, 900, 900, 870, 87, 549, 900, 535]\n",
      "20250415_piworm12_5.csv: [25, 5, 5, 44, 3, 31, 5, 8, 34, 8, 86, 280, 1, 60]\n",
      "20250415_piworm12_4.csv: [10, 106, 1, 63, 1, 82, 6, 1, 2, 14, 298, 35, 5, 16, 40, 7, 14]\n",
      "20250415_piworm10_6.csv: [24, 6, 34, 121, 218, 1, 103, 227, 15, 6, 22, 392, 1, 170]\n",
      "20250311_piworm19_2.csv: [83, 49, 12, 25, 6, 18, 11, 17, 37, 1, 33, 5, 70, 79, 52, 17, 13, 41, 35, 2, 2, 551, 900, 23, 302, 551, 350, 551, 900, 900, 900, 2]\n",
      "20250415_piworm10_4.csv: [2, 38, 21, 22, 41, 16, 2, 62, 36, 11, 56, 11, 10, 3, 13, 42, 6, 57, 7, 26, 40, 14, 197, 14, 35, 13, 25, 12, 20, 348, 7, 131, 900, 1]\n",
      "20250415_piworm12_6.csv: [900, 1, 84, 49, 38, 38, 45, 1]\n",
      "20250415_piworm10_5.csv: [88, 175, 15, 342, 4, 26, 2, 26, 2, 58, 348, 1, 65, 805, 21, 1, 482, 1, 900, 1]\n",
      "20240924_piworm12_2.csv: [28, 12, 3, 9, 26, 38, 94, 20, 33, 119, 50, 21, 1, 41, 69, 16, 12, 93, 45, 31]\n",
      "20250205_piworm12_3.csv: [15, 5, 4, 2]\n",
      "20250205_piworm10_1.csv: [10, 11, 24, 49, 2, 2, 7, 3, 98, 159, 20, 2, 10, 2, 45, 66, 16, 9, 191, 23, 7, 8, 65, 1, 46, 254, 75, 256, 75, 853]\n",
      "20240827_piworm12_3.csv: [106, 5, 10, 6, 13, 74, 11, 7, 8, 14, 10, 647, 219, 16, 6, 14, 12, 2, 68, 865, 854, 29, 33, 242, 505, 218, 252, 100, 179, 20, 301, 96, 734, 136, 524, 5, 12, 40, 16, 3, 2, 5, 3, 9, 2, 3, 5, 6, 3, 9, 2, 47, 22, 27, 147, 32, 6, 28, 26, 3, 10, 3, 28, 30, 42, 114, 4, 76, 11, 6, 3, 17, 21, 7, 10, 33, 4, 1, 24, 11, 11, 2, 9, 24, 11, 32, 12, 3, 40, 2, 22, 12, 40, 6, 3, 3, 10, 8, 16, 3, 10, 8, 3, 5, 3, 15, 8, 279, 135, 1, 2, 10, 7, 56, 13, 65, 20, 314, 169, 27, 2, 2, 2, 6, 2, 2, 3, 16, 12, 23, 2, 12, 227, 48, 4, 125, 5, 78, 44, 142, 900, 900, 900, 900, 69, 829, 900, 101, 17, 4, 172, 316, 900, 900, 900, 900, 168]\n",
      "20240924_piworm10_1.csv: [4, 12, 33, 12, 12, 12, 15, 5, 129, 62, 231, 41, 87, 149, 4, 81, 22, 11, 363, 335, 132, 66, 574, 57, 1, 11, 3, 20, 6, 39, 18, 177, 11, 277, 70, 191, 44, 284, 153, 64, 42, 1, 4, 38, 140, 41, 105, 4, 29, 182, 16, 12, 37, 24, 36, 23, 10, 15, 19, 13, 16, 900, 2]\n",
      "20240924_piworm12_3.csv: [4, 14, 85, 32, 57, 28, 25, 20, 15, 31, 70, 370, 42, 17, 13, 219, 100, 6, 105, 10, 76, 18, 8, 14, 108, 6, 192, 696, 13, 131, 31, 74, 30, 19, 31]\n",
      "20250205_piworm12_2.csv: [7, 2, 19, 12, 26, 46, 40, 19, 16, 15, 17, 29, 39, 4, 13, 12, 30, 2, 48, 80, 173, 1, 9, 16, 44, 2, 41, 32, 3, 8, 4, 5, 25, 3, 50, 346, 900, 482, 531, 900, 900, 900, 900, 900, 900, 900]\n",
      "20240827_piworm10_1.csv: [10, 7, 39, 2, 348, 193, 30, 20, 41, 7, 6, 2]\n",
      "20240924_piworm12_1.csv: [112, 124, 12, 19, 1, 2, 12, 119, 19, 12, 21, 6, 22, 3, 282, 746, 1, 32, 61, 12, 11, 11, 24, 5, 13, 23, 7, 10, 12, 46, 5, 9, 2, 37, 11, 25, 58, 3, 13, 12, 12]\n",
      "20240924_piworm10_3.csv: [420, 337, 3, 306, 87, 158, 1, 12, 205, 24, 13, 277, 49, 1, 879, 2, 22, 34, 3, 12, 2, 48, 22, 12, 1, 32, 21, 37, 11, 13, 15]\n",
      "20250205_piworm10_2.csv: [142, 147, 1, 32, 1, 2, 119, 81, 1, 2, 310, 91, 13]\n",
      "20240924_piworm10_2.csv: [11, 60, 18, 108, 1, 40, 180, 78, 121, 181, 2, 29, 13, 26, 1, 169, 1, 200, 73, 850, 1, 180, 1, 24, 616]\n",
      "20250205_piworm12_1.csv: [11, 8, 19, 35, 5, 6, 21, 72, 12, 140, 17, 13, 21, 2]\n",
      "20250205_piworm12_5.csv: [900, 1, 139, 5, 27, 285, 266, 4, 66, 32, 30, 1, 572, 32, 1, 25, 40, 32, 1, 30, 129, 2, 53, 2, 319]\n",
      "20240827_piworm12_4.csv: [6, 34, 88, 10, 5, 7, 14, 107, 69, 55, 2, 14, 22, 50, 3, 89, 141, 21, 14, 4, 110, 173, 151, 132, 180, 207, 900, 1, 12, 266, 104, 17, 2, 12, 40, 4, 18, 19, 34, 7, 114, 243, 261, 2, 24, 264]\n",
      "20240924_piworm10_6.csv: [30, 12, 53, 19, 10, 5, 7]\n",
      "20250205_piworm10_6.csv: [2, 2, 6, 11, 34, 2, 3, 10, 17, 17, 36, 2, 51, 136, 3, 200, 112, 24, 16, 32, 49, 1]\n",
      "20240924_piworm12_5.csv: [176, 563, 407, 54, 15, 42, 294, 28, 152, 39, 73, 128, 232, 10, 202, 313, 10, 17, 50, 16, 13, 36, 155, 245, 78, 45, 59, 61, 96, 18, 18, 19, 84, 12]\n",
      "20250205_piworm12_4.csv: [418, 12, 46, 28, 47, 6, 46, 8, 43, 71, 27, 40, 6, 24, 2, 95, 143, 106, 417, 236, 5]\n",
      "20240827_piworm10_5.csv: [14, 23, 6, 63, 198]\n",
      "20240924_piworm10_5.csv: [27, 129, 6, 10, 21, 13, 2, 35, 1]\n",
      "20250205_piworm10_4.csv: [900, 1, 20, 19, 69, 3, 23, 1, 900, 900, 900, 900, 340, 10, 16, 9]\n",
      "20240924_piworm10_4.csv: [213, 431, 19, 1, 470, 58, 2, 7, 900, 1, 10, 24, 744, 900, 900, 900, 856]\n",
      "20250205_piworm10_5.csv: [12, 371, 46, 11, 3, 11, 4, 2, 234, 16, 29, 14, 21, 16, 42, 4, 22, 8, 38, 54, 49]\n",
      "20240924_piworm12_6.csv: [11, 121, 94, 12, 25, 24, 6, 31, 2, 434, 116, 181, 1, 296, 12, 1, 23, 1, 6, 25, 51, 2]\n",
      "20250318_piworm12_3.csv: [55, 27, 13, 46, 13, 61, 17, 27, 23, 4, 2, 8, 26, 16, 109, 76, 7, 227, 38, 682, 1, 76, 19, 2, 248, 8, 6, 2, 18, 20, 20, 12, 164, 75, 2, 100, 21]\n",
      "20250318_piworm10_2.csv: [26, 12, 11, 97, 13, 6, 12, 1, 118, 63, 170, 7, 4, 9, 26, 37, 27, 49, 141, 472, 24, 28, 4, 56, 380, 1, 900, 679, 11, 561, 6, 632]\n",
      "20250318_piworm12_1.csv: [25, 21, 4, 23, 1, 7, 1, 41, 4, 28]\n",
      "20250318_piworm12_5.csv: [2, 6, 37, 9, 43, 11, 5, 132, 1, 37, 110, 216, 257, 28, 51, 61, 35, 18, 9]\n",
      "20250318_piworm12_6.csv: [47, 17, 65, 48, 73, 86, 23, 56, 12, 6, 92, 10, 6, 10, 834, 9, 144, 825, 74, 744, 336, 375, 1, 7, 47, 20, 51]\n",
      "20250318_piworm10_4.csv: [4, 49, 86, 395, 321, 116, 17, 20, 49, 11, 10, 49, 19, 14, 7]\n",
      "20250415_piworm11_2.csv: [10, 202, 191, 97, 12]\n",
      "20250415_piworm11_3.csv: [39, 42, 371, 23, 236, 10, 6, 9, 13, 25, 3, 8, 76, 121, 39, 8, 2, 26, 8, 6, 121, 83, 40, 6, 57, 352, 197, 2, 7, 2, 2, 2, 6, 3, 2, 2, 2, 2, 2, 2, 2, 2, 8, 6, 14, 2, 2, 2, 2, 3, 2, 53, 8, 35, 103, 85, 304, 31, 18, 36, 57, 79, 11, 12, 67, 35, 17, 131, 284, 166, 25, 29, 108, 19, 8, 12, 9, 38, 7, 27, 5, 39, 4, 213, 3, 97, 898]\n",
      "20250415_piworm11_1.csv: [26, 718, 72, 16, 900, 1, 73, 22, 192, 1, 101, 92]\n",
      "20250415_piworm11_4.csv: [5, 20, 14, 21, 51, 63, 24, 10, 7, 7, 65, 18, 21, 16, 16, 67, 900, 1, 880]\n",
      "20250415_piworm11_5.csv: [11, 896, 900, 1, 809, 195, 319]\n",
      "20250415_piworm11_6.csv: [621, 65, 3, 101, 439, 1, 9, 156, 96, 59, 22, 12, 18, 114, 258, 9, 392, 228, 20, 52, 78, 182, 221, 14, 9, 343, 48, 82]\n",
      "20250415_piworm09_1.csv: [779, 9, 12, 4, 46, 6, 11, 7, 4, 87, 6, 66, 7, 55, 27]\n",
      "20250311_piworm20_2.csv: [90, 366, 36, 28, 270, 43, 2, 12, 8, 26, 5, 50, 369, 105, 1, 110, 31, 65, 66, 19, 11, 7, 25, 27, 26, 11, 17, 5, 8, 126, 300, 109, 322, 23, 83, 34, 10, 596, 106, 644, 14, 8, 5, 234, 43, 587, 314, 49, 432, 314, 262, 578, 663, 900, 314, 587, 900, 900, 900]\n",
      "20250415_piworm09_3.csv: [32, 38, 41, 1, 10, 10, 446, 394, 1, 35, 267, 4, 7, 71, 104, 1, 131, 13, 2, 2, 3, 16, 13, 45, 22, 616, 900, 105, 129, 616, 285]\n",
      "20250415_piworm09_6.csv: [9, 62, 12, 101, 134, 14, 122, 250, 900, 900, 900, 900, 900, 900, 214, 66]\n",
      "20250311_piworm20_5.csv: [67, 888, 900, 637, 234, 900, 253, 137, 7, 244, 6, 16, 57, 2, 2, 222, 7, 236, 307, 11, 13, 216, 23, 93, 900, 1, 113, 51, 39, 230, 1, 387, 392, 15, 4, 19, 1, 134, 127, 16, 8, 75, 5, 1, 314, 144, 5, 199, 19, 178, 314, 586, 179, 5, 16, 32, 2, 22, 587, 22, 13, 73, 174, 88, 128, 303, 461, 91, 239, 21, 900, 900, 371, 576, 900, 900, 382, 587, 900, 900, 900]\n",
      "20250415_piworm09_5.csv: [451, 1, 900, 900, 64, 24, 548, 205, 900, 502, 74, 11, 159, 24, 126, 193, 58, 85, 86, 134]\n",
      "20250415_piworm09_4.csv: [27, 24, 21, 34, 63]\n",
      "20250318_piworm11_2.csv: [87, 82, 138, 49, 5, 44, 32, 17, 44, 21, 53, 18, 173, 36, 78, 9, 9, 1, 151, 1, 10, 13, 900, 1, 61]\n",
      "20240924_piworm09_1.csv: [34, 7, 2, 28, 103, 10, 33, 2, 31, 1, 82, 1, 1, 61, 15, 77, 30, 78, 32, 50, 61, 10, 12, 78, 4, 9, 10]\n",
      "20250205_piworm09_1.csv: [37, 12, 7, 900, 1, 900]\n",
      "20240827_piworm09_2.csv: [16, 4, 73, 5, 9, 8, 36, 898, 120, 141, 339, 900, 1, 142, 13, 64, 3, 481, 899, 1]\n",
      "20250205_piworm09_3.csv: [8, 33, 14, 153, 8]\n",
      "20240924_piworm09_2.csv: [900, 1, 38, 96, 1, 15, 26, 43, 26, 2, 143, 41, 76, 128, 20, 245, 23, 46, 168, 24, 229, 23, 14, 11, 85, 335, 287, 23, 26, 4, 48, 78, 50, 510, 85, 36, 900, 12, 322, 550, 58, 1, 143, 332]\n",
      "20250205_piworm09_2.csv: [323]\n",
      "20240924_piworm09_3.csv: [4, 1, 79, 899, 1, 725, 900, 1, 573, 45, 354, 507, 2, 7, 2, 50, 8, 521, 24, 121, 209, 296, 28, 10, 611, 900, 24, 13, 649, 36, 625, 1, 900, 900, 147, 746, 1, 900, 900, 900, 900, 900, 900, 900, 899]\n",
      "20250205_piworm09_6.csv: [7, 8, 88, 28]\n",
      "20240924_piworm09_6.csv: [60, 6, 200, 10, 147, 12, 703, 8, 27, 8, 5, 34, 37]\n",
      "20250318_piworm11_5.csv: [51, 38, 30, 27, 10, 122, 56, 37, 13, 4, 18, 9, 48, 132, 19, 255, 61, 10, 19, 41, 290, 36, 6, 19, 28, 10, 57, 14, 66, 13, 6, 2, 3, 2, 3, 24, 3, 31, 8, 7, 5, 12, 9, 7, 14, 126, 63, 38, 31, 9, 7, 3, 22, 48, 68, 19, 6]\n",
      "20250205_piworm09_5.csv: [7, 69, 15, 14, 22, 100, 35, 12, 156, 12, 1, 15, 211, 11, 4, 65, 46, 4, 44, 199, 13, 75, 119, 49]\n",
      "20240924_piworm09_4.csv: [2, 1, 1, 22, 25, 9, 22, 2, 23, 1, 55, 7, 11, 147]\n",
      "20250205_piworm09_4.csv: [320, 7, 230, 1, 11, 13, 3, 2, 6, 103, 149, 72, 171, 2, 19, 50, 131, 41, 197, 171, 2, 19, 50, 131, 41, 197, 1, 586, 106, 677, 89, 2]\n",
      "20240924_piworm09_5.csv: [19, 26, 48, 2, 17, 65, 72, 42, 5, 5, 32]\n",
      "20250311_piworm17_5.csv: [228, 37, 64, 122, 1, 17, 103, 49, 9, 52, 428, 469, 1, 3, 2, 4, 3, 15, 9, 11, 3, 3, 2, 9, 3, 4, 2, 2, 16, 3, 7, 10, 5, 22, 4, 3, 3, 5, 33, 7, 15, 3, 7, 20, 8, 9, 11, 3, 22, 7, 102, 340, 260, 17, 2, 165, 4, 20, 50, 8, 465, 900, 900, 900]\n",
      "20240827_piworm09_5.csv: [5, 372, 48, 11, 509, 780, 748, 228, 518, 7, 128, 14, 215, 204, 71, 35, 34, 13, 73, 13, 2, 85, 25, 39, 20, 13, 51, 80, 2, 39, 79, 576, 229, 1, 533, 697, 12, 99, 48, 112, 58, 897, 900, 900, 503, 310, 899]\n",
      "20250205_piworm11_2.csv: [2, 28, 16, 48, 3, 2, 10, 40, 3, 9, 58, 16, 26, 4, 14, 14, 13, 58, 290, 197, 1, 56, 18, 60, 4, 33, 186, 9, 1, 32, 80, 17, 135, 1]\n",
      "20240924_piworm11_3.csv: [4, 20, 163, 21, 7, 104, 2, 99, 21, 135, 156, 108, 203, 10, 48, 45, 29, 36, 22, 25, 12, 355, 1]\n",
      "20240827_piworm11_3.csv: [17, 27, 28, 8, 94, 76, 172, 6, 4, 86, 22, 15, 58, 14, 10, 15, 1, 11, 3, 130, 6]\n",
      "20240827_piworm11_2.csv: [12, 27, 15, 2, 8, 2, 12, 12, 106, 65, 53, 6, 1, 69, 318, 52, 6, 102, 14, 14, 33, 1, 5, 3, 9, 38, 22, 7, 310]\n",
      "20250318_piworm09_1.csv: [5, 112, 10, 296, 56, 1, 5, 8, 57, 9, 6, 37, 13, 5, 103, 180, 349, 1, 117, 384, 7, 264, 20, 17, 9, 450, 95, 899, 1, 190, 14, 30, 120, 75, 567, 25, 5, 38, 2, 157, 17, 62, 172, 1, 11, 9, 2, 2, 8, 36, 8, 23, 514, 5, 15, 11, 80, 69, 308, 440, 269, 215, 93, 1]\n",
      "20250205_piworm11_3.csv: [7, 40, 14]\n",
      "20240924_piworm11_2.csv: [130, 247, 58, 265, 900, 36, 234, 897, 529, 7, 59, 7, 20, 401, 30, 38, 67, 157, 64, 24]\n",
      "20250318_piworm09_3.csv: [21, 66, 109, 33, 1, 29, 3, 74, 26, 1, 553, 16, 649, 49, 1, 10, 66, 66, 35, 59, 25, 66, 244, 243, 30, 322, 74, 62, 42, 73, 290, 60, 2, 2, 6]\n",
      "20250318_piworm09_2.csv: [3, 529, 1, 20, 187, 192, 88, 21, 11, 140, 1, 35, 122, 15, 119, 26, 115, 23, 2, 235, 39, 109, 56, 33, 900, 1, 13, 5, 48, 88, 72, 900, 900, 899]\n",
      "20240827_piworm11_1.csv: [101, 34, 11, 13, 323, 20, 81, 35, 25, 2, 554, 25, 51, 98, 60, 20, 61, 257, 1, 81, 35, 57, 44, 612, 74, 12, 13, 6, 13, 2, 232, 7, 1, 385, 53, 6, 102, 14, 14, 33, 1, 5, 9, 39, 24, 9, 298, 900, 105, 34, 8, 65, 6, 310]\n",
      "20240924_piworm11_1.csv: [36, 495, 247, 1]\n",
      "20250205_piworm11_4.csv: [13, 6, 12, 8, 895, 13, 16, 9, 253, 1, 92, 83, 46, 84, 47, 20, 21, 69, 1, 115, 27, 49, 28, 23, 17, 17, 2, 6, 57, 3, 1, 202, 42, 244, 2, 899, 2, 899, 2]\n",
      "20240924_piworm11_5.csv: [785, 1, 246, 481, 46, 73, 49, 3, 4, 11, 223, 12, 16, 73, 20, 413]\n",
      "20250318_piworm09_6.csv: [37, 29, 41, 26, 13, 40, 94, 9, 42, 5, 23, 13, 63, 36, 2, 251, 1, 109, 266, 112, 563, 2, 14, 13, 14]\n",
      "20240827_piworm11_4.csv: [18, 101, 19, 30, 100, 1, 4, 10, 11, 61, 14, 31, 88, 49, 900, 900, 243, 154, 170, 172, 48, 422, 6, 40, 377, 839, 222, 251, 73, 228, 109, 504, 1, 142, 2]\n",
      "20250205_piworm11_5.csv: [7, 7, 3, 193, 85, 16, 12, 892, 139, 53, 2, 22, 35, 9, 2, 80, 74, 6, 14, 11, 12, 8, 26, 42, 8, 2]\n",
      "20240924_piworm11_4.csv: [12, 46, 116, 140, 2, 22, 33, 4, 15, 16, 108, 12, 191, 24, 71, 6, 166, 217, 104, 15, 2, 35, 348, 16, 84, 514, 13, 78]\n",
      "20240924_piworm11_6.csv: [41, 64, 121, 69, 52, 305, 7, 192, 757, 58, 48, 85, 19, 10, 40, 2, 243, 181, 107, 579, 1, 189, 529, 15, 900, 900, 900, 900]\n",
      "20250318_piworm09_5.csv: [31, 29, 29, 2, 3, 21, 44, 22, 16, 1, 93, 91, 16, 105, 50, 186, 45, 2, 8, 40, 185]\n",
      "20240827_piworm11_6.csv: [21, 6, 11, 43, 40, 18, 244, 175, 134, 168, 59, 17, 10]\n",
      "20250318_piworm09_4.csv: [12, 5, 2, 44, 29, 71, 43, 38, 19, 28, 57, 322, 284, 148, 1, 126, 14, 272, 1, 687, 10]\n",
      "20250205_piworm11_6.csv: [7, 5, 900, 720, 87, 324, 22, 105, 37, 30, 34, 26, 17, 35, 3]\n"
     ]
    }
   ],
   "source": [
    "# Find lenght of consecutive gap in our csv. This could be for only speed columns, for X or Y or for all of them\n",
    "def find_consecutive_gaps(df, column):\n",
    "    is_na = df[column].isna()\n",
    "    gap_lengths = []\n",
    "    current_gap_length = 0\n",
    "\n",
    "    for val in is_na:\n",
    "        if val:\n",
    "            current_gap_length += 1\n",
    "        else:\n",
    "            if current_gap_length > 0:\n",
    "                gap_lengths.append(current_gap_length)\n",
    "                current_gap_length = 0\n",
    "    if current_gap_length > 0:\n",
    "        gap_lengths.append(current_gap_length)\n",
    "\n",
    "    return gap_lengths\n",
    "\n",
    "gap_lengths_speed = {}\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    for file_name in os.listdir(treatment_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(treatment_dir, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            segments = df['Segment'].unique()\n",
    "            list_gap = []\n",
    "            for segment in segments:\n",
    "                segment_df = df[df['Segment'] == segment]\n",
    "                gaps = find_consecutive_gaps(segment_df, 'X')\n",
    "                list_gap.extend(gaps)\n",
    "            gap_lengths_speed[file_name] = list_gap\n",
    "print(\"Consecutive gap lengths in 'Speed' column for each file:\")\n",
    "for file_name, gaps in gap_lengths_speed.items():\n",
    "    print(f\"{file_name}: {gaps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2b85958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with Speed > 10 in each file:\n",
      "20250311_piworm18_4.csv: 4203\n",
      "20250311_piworm18_5.csv: 2325\n",
      "20250311_piworm18_6.csv: 1192\n",
      "20250311_piworm18_2.csv: 1680\n",
      "20250311_piworm18_1.csv: 1948\n",
      "20250415_piworm10_1.csv: 1724\n",
      "20250415_piworm12_3.csv: 2483\n",
      "20250415_piworm12_2.csv: 1263\n",
      "20250311_piworm19_6.csv: 2684\n",
      "20250311_piworm19_4.csv: 1880\n",
      "20250415_piworm10_2.csv: 2669\n",
      "20250415_piworm12_1.csv: 702\n",
      "20250415_piworm10_3.csv: 1652\n",
      "20250311_piworm19_5.csv: 2269\n",
      "20250415_piworm12_5.csv: 3917\n",
      "20250415_piworm12_4.csv: 1915\n",
      "20250415_piworm10_6.csv: 2678\n",
      "20250311_piworm19_2.csv: 2158\n",
      "20250415_piworm10_4.csv: 4755\n",
      "20250415_piworm12_6.csv: 2361\n",
      "20250415_piworm10_5.csv: 3312\n",
      "20240924_piworm12_2.csv: 1344\n",
      "20250205_piworm12_3.csv: 3151\n",
      "20250205_piworm10_1.csv: 4145\n",
      "20240827_piworm12_3.csv: 1920\n",
      "20240924_piworm10_1.csv: 720\n",
      "20240924_piworm12_3.csv: 980\n",
      "20250205_piworm12_2.csv: 4808\n",
      "20240827_piworm10_1.csv: 5952\n",
      "20240924_piworm12_1.csv: 1008\n",
      "20240924_piworm10_3.csv: 1177\n",
      "20250205_piworm10_2.csv: 4428\n",
      "20240924_piworm10_2.csv: 1831\n",
      "20250205_piworm12_1.csv: 2552\n",
      "20250205_piworm12_5.csv: 3703\n",
      "20240827_piworm12_4.csv: 1850\n",
      "20240924_piworm10_6.csv: 1158\n",
      "20250205_piworm10_6.csv: 4641\n",
      "20240924_piworm12_5.csv: 1295\n",
      "20250205_piworm12_4.csv: 3648\n",
      "20240827_piworm10_5.csv: 587\n",
      "20240924_piworm10_5.csv: 958\n",
      "20250205_piworm10_4.csv: 2223\n",
      "20240924_piworm10_4.csv: 1542\n",
      "20250205_piworm10_5.csv: 2764\n",
      "20240924_piworm12_6.csv: 1833\n",
      "20250318_piworm12_3.csv: 3094\n",
      "20250318_piworm10_2.csv: 3911\n",
      "20250318_piworm12_1.csv: 1528\n",
      "20250318_piworm12_5.csv: 2074\n",
      "20250318_piworm12_6.csv: 1853\n",
      "20250318_piworm10_4.csv: 1257\n",
      "20250415_piworm11_2.csv: 1295\n",
      "20250415_piworm11_3.csv: 3198\n",
      "20250415_piworm11_1.csv: 1751\n",
      "20250415_piworm11_4.csv: 3246\n",
      "20250415_piworm11_5.csv: 1391\n",
      "20250415_piworm11_6.csv: 2682\n",
      "20250415_piworm09_1.csv: 2299\n",
      "20250311_piworm20_2.csv: 2473\n",
      "20250415_piworm09_3.csv: 3985\n",
      "20250415_piworm09_6.csv: 3817\n",
      "20250311_piworm20_5.csv: 1627\n",
      "20250415_piworm09_5.csv: 1031\n",
      "20250415_piworm09_4.csv: 2268\n",
      "20250318_piworm11_2.csv: 2804\n",
      "20240924_piworm09_1.csv: 2251\n",
      "20250205_piworm09_1.csv: 4846\n",
      "20240827_piworm09_2.csv: 2612\n",
      "20250205_piworm09_3.csv: 1881\n",
      "20240924_piworm09_2.csv: 1401\n",
      "20250205_piworm09_2.csv: 1963\n",
      "20240924_piworm09_3.csv: 519\n",
      "20250205_piworm09_6.csv: 1072\n",
      "20240924_piworm09_6.csv: 1398\n",
      "20250318_piworm11_5.csv: 2835\n",
      "20250205_piworm09_5.csv: 2264\n",
      "20240924_piworm09_4.csv: 2048\n",
      "20250205_piworm09_4.csv: 2028\n",
      "20240924_piworm09_5.csv: 1124\n",
      "20250311_piworm17_5.csv: 2378\n",
      "20240827_piworm09_5.csv: 1059\n",
      "20250205_piworm11_2.csv: 2808\n",
      "20240924_piworm11_3.csv: 945\n",
      "20240827_piworm11_3.csv: 3702\n",
      "20240827_piworm11_2.csv: 3793\n",
      "20250318_piworm09_1.csv: 1739\n",
      "20250205_piworm11_3.csv: 2233\n",
      "20240924_piworm11_2.csv: 1441\n",
      "20250318_piworm09_3.csv: 2636\n",
      "20250318_piworm09_2.csv: 2534\n",
      "20240827_piworm11_1.csv: 6854\n",
      "20240924_piworm11_1.csv: 319\n",
      "20250205_piworm11_4.csv: 3869\n",
      "20240924_piworm11_5.csv: 665\n",
      "20250318_piworm09_6.csv: 2318\n",
      "20240827_piworm11_4.csv: 2332\n",
      "20250205_piworm11_5.csv: 4680\n",
      "20240924_piworm11_4.csv: 554\n",
      "20240924_piworm11_6.csv: 2515\n",
      "20250318_piworm09_5.csv: 4696\n",
      "20240827_piworm11_6.csv: 4133\n",
      "20250318_piworm09_4.csv: 2746\n",
      "20250205_piworm11_6.csv: 3782\n"
     ]
    }
   ],
   "source": [
    "# Count the number of speed > 10 in each file\n",
    "def count_high_speed_entries(file, speed_threshold=10):\n",
    "    df = pd.read_csv(file)\n",
    "    high_speed_count = (df['Speed'] > speed_threshold).sum()\n",
    "    return high_speed_count\n",
    "\n",
    "high_speed_counts = {}\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    for file_name in os.listdir(treatment_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(treatment_dir, file_name)\n",
    "            count = count_high_speed_entries(file_path, speed_threshold=10)\n",
    "            high_speed_counts[file_name] = count\n",
    "print(\"Number of entries with Speed > 10 in each file:\")\n",
    "for file_name, count in high_speed_counts.items():\n",
    "    print(f\"{file_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f0cd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sanity check each segment each worms based on:\n",
    "# - Difference between two consecutive timestamps should be less than 1 hour, or if there are missing timestamps, then flag the segment\n",
    "# - Find some statistical anomalies in the segments based on statistical properties (mean, std, etc.) of the coordinates  and of the speed within each segment\n",
    "# - Flag also all the missing values in the segments for X, Y and Speed\n",
    "\n",
    "def sanity_check_segment(file, segment):\n",
    "    df = pd.read_csv(file)\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    anomalies = {}\n",
    "    \n",
    "    seg_df = df[df['Segment'] == segment]\n",
    "\n",
    "    # Check for missing timestamps\n",
    "    if seg_df['Timestamp'].isnull().any():\n",
    "        anomalies[\"timestamp_na\"] = True\n",
    "    else:\n",
    "        anomalies[\"timestamp_na\"] = False\n",
    "    \n",
    "    # Calculate time differences\n",
    "    time_diffs = seg_df['Timestamp'].diff().dt.total_seconds().abs()\n",
    "    if (time_diffs > 3600).any():\n",
    "        anomalies[\"timestamp_diff\"] = True\n",
    "    else:\n",
    "        anomalies[\"timestamp_diff\"] = False\n",
    "\n",
    "    # Check for missing values in X, Y, Speed\n",
    "    for col in ['X', 'Y', 'Speed']:  # Assuming these columns exist\n",
    "        if col in seg_df.columns:\n",
    "            if seg_df[col].isnull().any():\n",
    "                anomalies[f\"{col}_na\"] = True\n",
    "            else:\n",
    "                anomalies[f\"{col}_na\"] = False\n",
    "\n",
    "    # Statistical checks, flag rows that exceed mean +/- 3*std\n",
    "    for col in ['X', 'Y', 'Speed']:\n",
    "        if col in seg_df.columns:\n",
    "            mean = seg_df[col].mean()\n",
    "            std = seg_df[col].std()\n",
    "            if ((seg_df[col] > mean + 3 * std) | (seg_df[col] < mean - 3 * std)).any():\n",
    "                anomalies[f\"{col}_stat_anomaly\"] = True\n",
    "            else:\n",
    "                anomalies[f\"{col}_stat_anomaly\"] = False\n",
    "        \n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a matrix where each line represents a worm and each column represents a segment. Each cell contains a dictionary of anomalies found in that segment.\n",
    "# All worms don't have the same number of segments so we can fill missing segments with None\n",
    "\n",
    "anomaly_matrix = {}\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    for file_name in os.listdir(treatment_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(treatment_dir, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            max_segment = df['Segment'].max()\n",
    "            worm_anomalies = {}\n",
    "            for segment in range(max_segment + 1):\n",
    "                anomalies = sanity_check_segment(file_path, segment)\n",
    "                worm_anomalies[segment] = anomalies\n",
    "            anomaly_matrix[file_name] = worm_anomalies\n",
    "\n",
    "# Save the results in an excel file\n",
    "anomaly_df = pd.DataFrame.from_dict({(i,j): anomaly_matrix[i][j] \n",
    "                           for i in anomaly_matrix.keys() \n",
    "                           for j in anomaly_matrix[i].keys()}, orient='index')\n",
    "anomaly_df.to_excel('segment_anomalies.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6be99947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that the first row of each csv is missing speed value, so we can assume it to 0\n",
    "# We can also scale the coordinates to be between 0 and 1 based on the min and max values of X and Y in each file\n",
    "# We can also clip the speed that could be weird"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
