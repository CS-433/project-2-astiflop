{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7027de26"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "Run the following cell to mount your Google Drive. You will be prompted to authorize this notebook to access your Drive files. Follow the instructions in the output to complete the authorization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dbb72fe",
        "outputId": "6c0918b7-cd88-4540-ea98-9c956e0f9d9e"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9bc3fd3"
      },
      "source": [
        "### Explore your Google Drive\n",
        "\n",
        "After successfully mounting, you can explore your Drive files by running shell commands like `!ls /content/drive/MyDrive/`. This will list the contents of your 'My Drive' folder. Once you know the path to your data file, please let me know its full path and type (e.g., CSV, Excel, Parquet) so I can help you load it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0050ba6b",
        "outputId": "bd8aa2d8-48a2-4192-c16b-b72b3aa34cf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "worm_trajectories.zip\n"
          ]
        }
      ],
      "source": [
        "# Example: List contents of your My Drive folder\n",
        "!ls /content/drive/MyDrive/Project\\ CElegans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uhKm2-qddKMD"
      },
      "outputs": [],
      "source": [
        "zip_path = \"/content/drive/MyDrive/Project CElegans/worm_trajectories.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX3JZcezdk5p",
        "outputId": "06ebbad4-581a-476b-ddf8-860611781ff1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Décompression en cours...\n",
            "replace /content/worm/worm_trajectories/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/worm/__MACOSX/worm_trajectories/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/worm/worm_trajectories/TERBINAFINE- (control)/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/worm/__MACOSX/worm_trajectories/TERBINAFINE- (control)/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/worm/worm_trajectories/TERBINAFINE+/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/worm/__MACOSX/worm_trajectories/TERBINAFINE+/._.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/worm/worm_trajectories/TERBINAFINE- (control)/20250311_piworm18_4/20250311_piworm18_4_seg_49.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/worm/worm_trajectories/TERBINAFINE- (control)/20250311_piworm18_4/20250311_piworm18_4_seg_61.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/worm/worm_trajectories/TERBINAFINE- (control)/20250311_piworm18_4/20250311_piworm18_4_seg_75.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "replace /content/worm/worm_trajectories/TERBINAFINE- (control)/20250311_piworm18_4/20250311_piworm18_4_seg_74.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "Terminé ! Vos données sont prêtes dans /content/worm\n"
          ]
        }
      ],
      "source": [
        "print(\"Décompression en cours...\")\n",
        "!unzip -q \"$zip_path\" -d \"/content/worm\"\n",
        "print(\"Terminé ! Vos données sont prêtes dans /content/worm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "thmvxoateEKM",
        "outputId": "b69e9eff-d806-4ab4-c9b1-e5a259d0880b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entraînement sur : cuda avec le modèle densenet\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ERREUR FATALE: Le dossier de données n'a été trouvé ni à 'worm_trajectories' (Local) ni à '/worm_trajectories' (Colab).",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2824751473.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         )\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_data_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# ==========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2824751473.py\u001b[0m in \u001b[0;36mset_data_dir\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCOLAB_DATA_DIR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         raise FileNotFoundError(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;34mf\"ERREUR FATALE: Le dossier de données n'a été trouvé ni à '{DEFAULT_DATA_DIR}' (Local) ni à '{COLAB_DATA_DIR}' (Colab).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         )\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: ERREUR FATALE: Le dossier de données n'a été trouvé ni à 'worm_trajectories' (Local) ni à '/worm_trajectories' (Colab)."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# ==========================================\n",
        "# 1. CONFIGURATION\n",
        "# ==========================================\n",
        "# CHOISISSEZ VOTRE ARCHITECTURE ICI : \"resnet\", \"efficientnet\", ou \"densenet\"\n",
        "MODEL_NAME = \"densenet\"\n",
        "\n",
        "# Chemin par défaut si les données sont dans le même dossier que ce script\n",
        "DEFAULT_DATA_DIR = Path(\"worm_trajectories\")\n",
        "# Chemin Colab/système de fichiers temporaire si les données sont décompressées là\n",
        "COLAB_DATA_DIR = Path(\"/worm_trajectories\")\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 15\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Détection automatique du matériel\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Entraînement sur : {device} avec le modèle {MODEL_NAME}\")\n",
        "\n",
        "\n",
        "def set_data_dir():\n",
        "    \"\"\"Vérifie le chemin de DATA_DIR et s'adapte à l'environnement Colab.\"\"\"\n",
        "    if DEFAULT_DATA_DIR.exists():\n",
        "        print(f\"Dossier de données trouvé : {DEFAULT_DATA_DIR}\")\n",
        "        return DEFAULT_DATA_DIR\n",
        "    elif COLAB_DATA_DIR.exists():\n",
        "        print(f\"Dossier de données Colab trouvé : {COLAB_DATA_DIR}\")\n",
        "        return COLAB_DATA_DIR\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            f\"ERREUR FATALE: Le dossier de données n'a été trouvé ni à '{DEFAULT_DATA_DIR}' (Local) ni à '{COLAB_DATA_DIR}' (Colab).\"\n",
        "        )\n",
        "\n",
        "DATA_DIR = set_data_dir()\n",
        "\n",
        "# ==========================================\n",
        "# 2. PRÉPARATION DES DONNÉES\n",
        "# ==========================================\n",
        "\n",
        "class WormDataset(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "        self.classes = sorted([d.name for d in DATA_DIR.iterdir() if d.is_dir()])\n",
        "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
        "        print(f\"Classes détectées : {self.class_to_idx}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        label_str = img_path.parent.parent.name\n",
        "        if label_str not in self.class_to_idx:\n",
        "            raise ValueError(f\"Label '{label_str}' non trouvé dans le mapping de classe: {self.class_to_idx}\")\n",
        "        label = self.class_to_idx[label_str]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "def split_dataset_by_worm(root_dir, val_split=0.2):\n",
        "    \"\"\"Sépare les données par ver pour éviter le Data Leakage.\"\"\"\n",
        "    train_files = []\n",
        "    val_files = []\n",
        "\n",
        "    for class_dir in root_dir.iterdir():\n",
        "        if not class_dir.is_dir(): continue\n",
        "        worm_dirs = [d for d in class_dir.iterdir() if d.is_dir()]\n",
        "\n",
        "        if not worm_dirs:\n",
        "            continue\n",
        "\n",
        "        random.shuffle(worm_dirs)\n",
        "        split_idx = int(len(worm_dirs) * (1 - val_split))\n",
        "        train_worms = worm_dirs[:split_idx]\n",
        "        val_worms = worm_dirs[split_idx:]\n",
        "\n",
        "        for w in train_worms:\n",
        "            train_files.extend(list(w.glob(\"*.png\")))\n",
        "        for w in val_worms:\n",
        "            val_files.extend(list(w.glob(\"*.png\")))\n",
        "\n",
        "    print(f\"Split terminé : {len(train_files)} images d'entraînement, {len(val_files)} images de validation.\")\n",
        "    if len(train_files) == 0 or len(val_files) == 0:\n",
        "         raise ValueError(\"ERREUR: Le split n'a produit aucune image.\")\n",
        "    return train_files, val_files\n",
        "\n",
        "# ==========================================\n",
        "# 3. INITIALISATION DU MODÈLE\n",
        "# ==========================================\n",
        "\n",
        "def initialize_model(model_name, num_classes, use_pretrained=True):\n",
        "    model = None\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        model = models.resnet18(pretrained=use_pretrained)\n",
        "        num_ftrs = model.fc.in_features\n",
        "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    elif model_name == \"efficientnet\":\n",
        "        try:\n",
        "            model = models.efficientnet_b0(pretrained=use_pretrained)\n",
        "            num_ftrs = model.classifier[1].in_features\n",
        "            model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "        except:\n",
        "             print(\"Fallback to ResNet due to torchvision version.\")\n",
        "             return initialize_model(\"resnet\", num_classes, use_pretrained)\n",
        "    elif model_name == \"densenet\":\n",
        "        model = models.densenet121(pretrained=use_pretrained)\n",
        "        num_ftrs = model.classifier.in_features\n",
        "        model.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "    else:\n",
        "        print(\"Modèle invalide.\")\n",
        "        exit()\n",
        "\n",
        "    return model\n",
        "\n",
        "# ==========================================\n",
        "# 4. ÉVALUATION PAR VOTE (Nouveau)\n",
        "# ==========================================\n",
        "\n",
        "def evaluate_by_vote(model, dataset, device):\n",
        "    \"\"\"\n",
        "    Regroupe les prédictions par ID de ver et applique un vote majoritaire.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- DÉBUT DE L'ÉVALUATION PAR VOTE (Worm-Level) ---\")\n",
        "    model.eval()\n",
        "\n",
        "    # Dictionnaire pour stocker les votes : {worm_id: {'votes': [], 'true_label': int}}\n",
        "    worm_results = defaultdict(lambda: {'votes': [], 'true_label': None})\n",
        "\n",
        "    # On n'utilise pas de DataLoader mélangé ici pour pouvoir mapper fichier -> ver facilement\n",
        "    # Mais le dataset est accessible par index\n",
        "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    all_preds_raw = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(loader):\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Récupérer les infos fichiers pour ce batch\n",
        "            # Le loader charge dans l'ordre du dataset\n",
        "            start_idx = i * BATCH_SIZE\n",
        "            end_idx = start_idx + inputs.size(0)\n",
        "            batch_files = dataset.file_list[start_idx:end_idx]\n",
        "\n",
        "            for f_path, pred, label in zip(batch_files, predicted.cpu().numpy(), labels.numpy()):\n",
        "                # Structure: .../LABEL/WORM_ID/image.png\n",
        "                worm_id = f_path.parent.name\n",
        "                worm_results[worm_id]['votes'].append(pred)\n",
        "                worm_results[worm_id]['true_label'] = label\n",
        "\n",
        "    # Agrégation des votes\n",
        "    final_preds = []\n",
        "    final_labels = []\n",
        "    worm_ids = []\n",
        "\n",
        "    correct_worms = 0\n",
        "    total_worms = 0\n",
        "\n",
        "    for w_id, data in worm_results.items():\n",
        "        votes = data['votes']\n",
        "        true_label = data['true_label']\n",
        "\n",
        "        # Vote majoritaire\n",
        "        # Si la moyenne > 0.5, alors la classe prédite est 1, sinon 0\n",
        "        vote_score = sum(votes) / len(votes)\n",
        "        final_pred = 1 if vote_score >= 0.5 else 0\n",
        "\n",
        "        final_preds.append(final_pred)\n",
        "        final_labels.append(true_label)\n",
        "        worm_ids.append(w_id)\n",
        "\n",
        "        if final_pred == true_label:\n",
        "            correct_worms += 1\n",
        "        total_worms += 1\n",
        "\n",
        "    # Affichage des résultats\n",
        "    accuracy = correct_worms / total_worms if total_worms > 0 else 0\n",
        "    print(f\"\\n>>> PRÉCISION PAR VER (VOTE MAJORITAIRE) : {accuracy:.2%}\")\n",
        "    print(f\"Total Vers testés : {total_worms}\")\n",
        "\n",
        "    print(\"\\nRapport de Classification (Niveau Ver) :\")\n",
        "    print(classification_report(final_labels, final_preds, target_names=dataset.classes))\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "# ==========================================\n",
        "# 5. PIPELINE PRINCIPAL\n",
        "# ==========================================\n",
        "\n",
        "def train_model():\n",
        "    try:\n",
        "        train_files, val_files = split_dataset_by_worm(DATA_DIR)\n",
        "    except ValueError as e:\n",
        "        print(f\"Échec : {e}\")\n",
        "        return\n",
        "\n",
        "    data_transforms = transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dataset = WormDataset(train_files, transform=data_transforms)\n",
        "    val_dataset = WormDataset(val_files, transform=data_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    num_classes = len(train_dataset.classes)\n",
        "    print(f\"Chargement de {MODEL_NAME}...\")\n",
        "    model = initialize_model(MODEL_NAME, num_classes, use_pretrained=True)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "\n",
        "    history = {'train_acc': [], 'val_acc': [], 'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    print(f\"\\n--- Début de l'entraînement ({num_classes} classes) ---\")\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_dataset)\n",
        "        epoch_acc = correct / total\n",
        "\n",
        "        # Validation classique (Segment Level)\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_epoch_loss = val_loss / len(val_dataset)\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        history['train_acc'].append(epoch_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['train_loss'].append(epoch_loss)\n",
        "        history['val_loss'].append(val_epoch_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {epoch_loss:.4f} | Val Acc (Segment): {val_acc:.4f}\")\n",
        "\n",
        "    # Sauvegarde\n",
        "    torch.save(model.state_dict(), f\"worm_classifier_{MODEL_NAME}.pth\")\n",
        "\n",
        "    # APPEL DE L'ÉVALUATION PAR VOTE À LA FIN\n",
        "    evaluate_by_vote(model, val_dataset, device)\n",
        "\n",
        "    # Graphiques\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history['train_acc'], label='Train')\n",
        "    plt.plot(history['val_acc'], label='Val (Seg)')\n",
        "    plt.title('Précision (Segment)')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history['train_loss'], label='Train')\n",
        "    plt.plot(history['val_loss'], label='Val')\n",
        "    plt.title('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'training_history_{MODEL_NAME}.png')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
