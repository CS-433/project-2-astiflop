{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2dc2436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "CONTROL = 'TERBINAFINE- (control)'\n",
    "TREATED = 'TERBINAFINE+'\n",
    "PREPROCESSED_DIR = 'data/preprocessed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b8d41e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whethen X is NaN if also Y is NaN\n",
    "def check_nan_consistency(segment_df):\n",
    "    \"\"\"\n",
    "    Check that for each row in the segment dataframe, if X is NaN then Y is also NaN,\n",
    "    and vice versa.\n",
    "\n",
    "    Args:\n",
    "        segment_df (pandas.DataFrame): DataFrame containing 'X' and 'Y' columns.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the condition holds for all rows, False otherwise.\n",
    "    \"\"\"\n",
    "    for index, row in segment_df.iterrows():\n",
    "        x_is_nan = pd.isna(row['X'])\n",
    "        y_is_nan = pd.isna(row['Y'])\n",
    "        if x_is_nan != y_is_nan:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "for file in os.listdir('data/'+CONTROL):\n",
    "    df = pd.read_csv(os.path.join('data/'+CONTROL, file))\n",
    "    assert check_nan_consistency(df), f\"Inconsistent NaN values in file {file} of CONTROL group.\"\n",
    "for file in os.listdir('data/'+TREATED):\n",
    "    df = pd.read_csv(os.path.join('data/'+TREATED, file))\n",
    "    assert check_nan_consistency(df), f\"Inconsistent NaN values in file {file} of TREATED group.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new files and Rename files according to their worm ID. For example, coordinates_highestspeed_20231010_12_01_...csv should be renamed to 20231010_piworm12_1.csv\n",
    "# Save the renamed files in PREPROCESSED_DIR/CONTROL and PREPROCESSED_DIR/TREATED\n",
    "\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    os.makedirs(treatment_dir, exist_ok=True)\n",
    "    for file_name in os.listdir('data/'+treatment):\n",
    "        if file_name.endswith('.csv'):\n",
    "            parts = file_name.split('_')\n",
    "            if len(str(parts[3])) == 1:\n",
    "                parts[3] = '0' + str(parts[3])\n",
    "            worm_id = f\"{parts[2]}_piworm{parts[3]}_{parts[4]}\"\n",
    "            df = pd.read_csv(os.path.join('data', treatment, file_name))\n",
    "            df.to_csv(os.path.join(treatment_dir, f\"{worm_id}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66acb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File counts match between original and processed directories.\n"
     ]
    }
   ],
   "source": [
    "# Check that we have the same number of files in PREPROCESSED_DIR/CONTROL and PREPROCESSED_DIR/TREATED as in the original directories\n",
    "original_control_files = [f for f in os.listdir('data/'+CONTROL) if f.endswith('.csv')]\n",
    "original_treated_files = [f for f in os.listdir('data/'+TREATED) if f.endswith('.csv')]\n",
    "processed_control_files = [f for f in os.listdir(os.path.join(PREPROCESSED_DIR, CONTROL)) if f.endswith('.csv')]\n",
    "processed_treated_files = [f for f in os.listdir(os.path.join(PREPROCESSED_DIR, TREATED)) if f.endswith('.csv')]\n",
    "assert len(original_control_files) == len(processed_control_files), \"Mismatch in CONTROL files count\"\n",
    "assert len(original_treated_files) == len(processed_treated_files), \"Mismatch in TREATED files count\"\n",
    "print(\"File counts match between original and processed directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11614226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the first row if it contains NaN in 'Timestamp' column or if the the absolute difference between the first two timestamps is larger than 1 hours\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    for file_name in os.listdir(treatment_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(treatment_dir, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "            if pd.isna(df.loc[0, 'Timestamp']) or abs((df.loc[1, 'Timestamp'] - df.loc[0, 'Timestamp']).total_seconds()) > 3600:\n",
    "                df = df.drop(index=0).reset_index(drop=True)\n",
    "                df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c1142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files failing sanity check: 0\n"
     ]
    }
   ],
   "source": [
    "# Sanity check the timestamp of the beginning of our dfs\n",
    "def sanity_check_first_timestamps(file):\n",
    "    df = pd.read_csv(file)\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    if len(df) >= 2:\n",
    "        if df.loc[0, 'Timestamp'] is pd.NaT:    \n",
    "            time_diff = abs((df.loc[2, 'Timestamp'] - df.loc[1, 'Timestamp']).total_seconds())\n",
    "        else:\n",
    "            time_diff = abs((df.loc[1, 'Timestamp'] - df.loc[0, 'Timestamp']).total_seconds())\n",
    "        if time_diff > 3600:  # 1 hour in seconds\n",
    "            return False\n",
    "        elif time_diff <= 3600:\n",
    "            return True\n",
    "        elif time_diff is pd.NaT:\n",
    "            return False\n",
    "        print(time_diff)\n",
    "    return False\n",
    "\n",
    "insanity_counts = 0\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    for file_name in os.listdir(treatment_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(treatment_dir, file_name)\n",
    "            if not sanity_check_first_timestamps(file_path):\n",
    "                print(f\"Sanity check failed for {file_name}\")\n",
    "                insanity_counts += 1\n",
    "print(f\"Total files failing sanity check: {insanity_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd883acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing timestamps found in TERBINAFINE- (control)/20250205_piworm10_2.csv\n",
      "Missing timestamps found in TERBINAFINE- (control)/20240924_piworm12_5.csv\n",
      "Missing timestamps found in TERBINAFINE+/20250205_piworm09_5.csv\n",
      "Missing timestamps found in TERBINAFINE+/20250205_piworm11_5.csv\n",
      "Total files with missing timestamps: 4\n",
      "Files with missing timestamps: ['data/processed/TERBINAFINE- (control)/20250205_piworm10_2.csv', 'data/processed/TERBINAFINE- (control)/20240924_piworm12_5.csv', 'data/processed/TERBINAFINE+/20250205_piworm09_5.csv', 'data/processed/TERBINAFINE+/20250205_piworm11_5.csv']\n"
     ]
    }
   ],
   "source": [
    "# See in which file there are missing timestamps\n",
    "count_missing_timestamps = 0\n",
    "file_with_missing_timestamps = []\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    for file_name in os.listdir(treatment_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(treatment_dir, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "            if df['Timestamp'].isnull().any():\n",
    "                print(f\"Missing timestamps found in {treatment}/{file_name}\")\n",
    "                count_missing_timestamps += 1\n",
    "                file_with_missing_timestamps.append(treatment_dir+'/'+file_name)\n",
    "                continue\n",
    "print(f\"Total files with missing timestamps: {count_missing_timestamps}\")\n",
    "print(f\"Files with missing timestamps: {file_with_missing_timestamps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c023e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Segment column to all files based on fixed frame counts.\n"
     ]
    }
   ],
   "source": [
    "# Add Segment column to our DF. A segment is composed of exactly 900 frames. Hence the first 900 frames are segment 0, the next 900 frames are segment 1, and so on.\n",
    "\n",
    "def add_segment_column_fixed_frames(file, frames_per_segment=900):\n",
    "    df = pd.read_csv(file)\n",
    "    num_rows = len(df)\n",
    "    df['Segment'] = np.arange(num_rows) // frames_per_segment\n",
    "    df.to_csv(file, index=False)\n",
    "\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    for file_name in os.listdir(treatment_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(treatment_dir, file_name)\n",
    "            add_segment_column_fixed_frames(file_path, frames_per_segment=900)\n",
    "\n",
    "print(\"Added Segment column to all files based on fixed frame counts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ae6b51",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     24\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(treatment_dir, file_name)\n\u001b[0;32m---> 25\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     segments \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSegment\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m     27\u001b[0m     list_gap \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Find lenght of consecutive gap in our csv. This could be for only speed columns, for X or Y or for all of them\n",
    "def find_consecutive_gaps(df, column):\n",
    "    is_na = df[column].isna()\n",
    "    gap_lengths = []\n",
    "    current_gap_length = 0\n",
    "\n",
    "    for val in is_na:\n",
    "        if val:\n",
    "            current_gap_length += 1\n",
    "        else:\n",
    "            if current_gap_length > 0:\n",
    "                gap_lengths.append(current_gap_length)\n",
    "                current_gap_length = 0\n",
    "    if current_gap_length > 0:\n",
    "        gap_lengths.append(current_gap_length)\n",
    "\n",
    "    return gap_lengths\n",
    "\n",
    "gap_lengths_speed = {}\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    for file_name in os.listdir(treatment_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(treatment_dir, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            segments = df['Segment'].unique()\n",
    "            list_gap = []\n",
    "            for segment in segments:\n",
    "                segment_df = df[df['Segment'] == segment]\n",
    "                gaps = find_consecutive_gaps(segment_df, 'X')\n",
    "                list_gap.extend(gaps)\n",
    "            gap_lengths_speed[file_name] = list_gap\n",
    "print(\"Consecutive gap lengths in 'Speed' column for each file:\")\n",
    "for file_name, gaps in gap_lengths_speed.items():\n",
    "    print(f\"{file_name}: {gaps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2b85958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries with Speed > 10 in each file:\n",
      "20250311_piworm18_4.csv: 4.864639637032836\n",
      "20250311_piworm18_5.csv: 2.6910033680945378\n",
      "20250311_piworm18_6.csv: 1.379645597749974\n",
      "20250311_piworm18_2.csv: 1.9444669498489566\n",
      "20250311_piworm18_1.csv: 2.2546557251820047\n",
      "20250415_piworm10_1.csv: 3.192651715772514\n",
      "20250415_piworm12_3.csv: 3.284434979298668\n",
      "20250415_piworm12_2.csv: 1.949104152841865\n",
      "20250311_piworm19_6.csv: 4.1420392290004475\n",
      "20250311_piworm19_4.csv: 1.7407568588598044\n",
      "20250415_piworm10_2.csv: 3.5304699797616372\n",
      "20250415_piworm12_1.csv: 1.3000240745198985\n",
      "20250415_piworm10_3.csv: 2.185214090133467\n",
      "20250311_piworm19_5.csv: 1.8621102822299365\n",
      "20250415_piworm12_5.csv: 5.181285466738978\n",
      "20250415_piworm12_4.csv: 2.533102289712827\n",
      "20250415_piworm10_6.csv: 3.5423748991388773\n",
      "20250311_piworm19_2.csv: 1.998166649691201\n",
      "20250415_piworm10_4.csv: 7.338076204879705\n",
      "20250415_piworm12_6.csv: 3.1230571832960754\n",
      "20250415_piworm10_5.csv: 4.381010330824481\n",
      "20240924_piworm12_2.csv: 2.0741060818839796\n",
      "20250205_piworm12_3.csv: 3.2418028992067818\n",
      "20250205_piworm10_1.csv: 6.396703652834149\n",
      "20240827_piworm12_3.csv: 2.963008688405685\n",
      "20240924_piworm10_1.csv: 1.333358025148614\n",
      "20240924_piworm12_3.csv: 1.2963134432995147\n",
      "20250205_piworm12_2.csv: 5.564879223139156\n",
      "20240827_piworm10_1.csv: 11.022426341228542\n",
      "20240924_piworm12_1.csv: 1.5555795614129848\n",
      "20240924_piworm10_3.csv: 2.179669993888776\n",
      "20250205_piworm10_2.csv: 3.418196415062296\n",
      "20240924_piworm10_2.csv: 2.8256608898285465\n",
      "20250205_piworm12_1.csv: 2.9537378904848435\n",
      "20250205_piworm12_5.csv: 4.898212939324594\n",
      "20240827_piworm12_4.csv: 2.854982329974228\n",
      "20240924_piworm10_6.csv: 1.787064615194679\n",
      "20250205_piworm10_6.csv: 5.371589948957743\n",
      "20240924_piworm12_5.csv: 1.4988599405085705\n",
      "20250205_piworm12_4.csv: 5.6297165079708025\n",
      "20240827_piworm10_5.csv: 1.358827750642376\n",
      "20240924_piworm10_5.csv: 1.2672125292662602\n",
      "20250205_piworm10_4.csv: 2.0583523921517792\n",
      "20240924_piworm10_4.csv: 2.379666352875816\n",
      "20250205_piworm10_5.csv: 3.6561330176325084\n",
      "20240924_piworm12_6.csv: 2.4246352464979695\n",
      "20250318_piworm12_3.csv: 4.092646728131324\n",
      "20250318_piworm10_2.csv: 3.9631149617469728\n",
      "20250318_piworm12_1.csv: 1.5720326340805977\n",
      "20250318_piworm12_5.csv: 2.743422532044075\n",
      "20250318_piworm12_6.csv: 2.14470074885126\n",
      "20250318_piworm10_4.csv: 1.6627204063545813\n",
      "20250415_piworm11_2.csv: 1.9984876309819595\n",
      "20250415_piworm11_3.csv: 4.230214685379436\n",
      "20250415_piworm11_1.csv: 2.70220219447831\n",
      "20250415_piworm11_4.csv: 5.009336563835862\n",
      "20250415_piworm11_5.csv: 1.6099723376427968\n",
      "20250415_piworm11_6.csv: 3.5476659744176513\n",
      "20250415_piworm09_1.csv: 3.0659874106476046\n",
      "20250311_piworm20_2.csv: 3.271207291101734\n",
      "20250415_piworm09_3.csv: 6.1497862621336745\n",
      "20250415_piworm09_6.csv: 3.8099896191008544\n",
      "20250311_piworm20_5.csv: 2.1521448696411327\n",
      "20250415_piworm09_5.csv: 1.363774653103877\n",
      "20250415_piworm09_4.csv: 2.3333573390672746\n",
      "20250318_piworm11_2.csv: 3.7090437704202435\n",
      "20240924_piworm09_1.csv: 4.168595714735458\n",
      "20250205_piworm09_1.csv: 4.4870785840609635\n",
      "20240827_piworm09_2.csv: 3.4550721570391145\n",
      "20250205_piworm09_3.csv: 3.483397840700754\n",
      "20240924_piworm09_2.csv: 2.1620704023210235\n",
      "20250205_piworm09_2.csv: 1.8176094223094656\n",
      "20240924_piworm09_3.csv: 0.8009382860846619\n",
      "20250205_piworm09_6.csv: 1.4180081747113058\n",
      "20240924_piworm09_6.csv: 2.5889368321635584\n",
      "20250318_piworm11_5.csv: 4.37506751647402\n",
      "20250205_piworm09_5.csv: 1.7469270596223736\n",
      "20240924_piworm09_4.csv: 3.7926628270893907\n",
      "20250205_piworm09_4.csv: 3.1296779271285056\n",
      "20240924_piworm09_5.csv: 1.7345946696708283\n",
      "20250311_piworm17_5.csv: 2.7523466706790587\n",
      "20240827_piworm09_5.csv: 1.9611474286560862\n",
      "20250205_piworm11_2.csv: 3.250037616176113\n",
      "20240924_piworm11_3.csv: 2.187550637746244\n",
      "20240827_piworm11_3.csv: 6.855682512639123\n",
      "20240827_piworm11_2.csv: 7.02420415192874\n",
      "20250318_piworm09_1.csv: 2.683683390175774\n",
      "20250205_piworm11_3.csv: 2.5845206541742383\n",
      "20240924_piworm11_2.csv: 1.9061098691781637\n",
      "20250318_piworm09_3.csv: 3.486818608711755\n",
      "20250318_piworm09_2.csv: 3.3518961891030306\n",
      "20240827_piworm11_1.csv: 12.69282764495639\n",
      "20240924_piworm11_1.csv: 0.4922915477090696\n",
      "20250205_piworm11_4.csv: 3.5824405781535016\n",
      "20240924_piworm11_5.csv: 0.8796412650960992\n",
      "20250318_piworm09_6.csv: 3.06617812404926\n",
      "20240827_piworm11_4.csv: 3.598820969459405\n",
      "20250205_piworm11_5.csv: 3.6142345236624243\n",
      "20240924_piworm11_4.csv: 0.7328139261101337\n",
      "20240924_piworm11_6.csv: 3.8812327350730724\n",
      "20250318_piworm09_5.csv: 6.211722377280123\n",
      "20240827_piworm11_6.csv: 5.467003531792749\n",
      "20250318_piworm09_4.csv: 3.6323231788780275\n",
      "20250205_piworm11_6.csv: 3.890986532783259\n"
     ]
    }
   ],
   "source": [
    "# Count the number of speed > 10 in each file\n",
    "def count_high_speed_entries(file, speed_threshold=10):\n",
    "    df = pd.read_csv(file)\n",
    "    high_speed_count = (df['Speed'] > speed_threshold).sum()/len(df)*100\n",
    "    return high_speed_count\n",
    "\n",
    "high_speed_counts = {}\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    for file_name in os.listdir(treatment_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(treatment_dir, file_name)\n",
    "            count = count_high_speed_entries(file_path, speed_threshold=10)\n",
    "            high_speed_counts[file_name] = count\n",
    "print(\"Number of entries with Speed > 10 in each file:\")\n",
    "for file_name, count in high_speed_counts.items():\n",
    "    print(f\"{file_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f0cd1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sanity check each segment each worms based on:\n",
    "# - Difference between two consecutive timestamps should be less than 1 hour, or if there are missing timestamps, then flag the segment\n",
    "# - Find some statistical anomalies in the segments based on statistical properties (mean, std, etc.) of the coordinates  and of the speed within each segment\n",
    "# - Flag also all the missing values in the segments for X, Y and Speed\n",
    "\n",
    "def sanity_check_segment(file, segment):\n",
    "    df = pd.read_csv(file)\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    anomalies = {}\n",
    "    \n",
    "    seg_df = df[df['Segment'] == segment]\n",
    "\n",
    "    # Check for missing timestamps\n",
    "    if seg_df['Timestamp'].isnull().any():\n",
    "        anomalies[\"timestamp_na\"] = True\n",
    "    else:\n",
    "        anomalies[\"timestamp_na\"] = False\n",
    "    \n",
    "    # Calculate time differences\n",
    "    time_diffs = seg_df['Timestamp'].diff().dt.total_seconds().abs()\n",
    "    if (time_diffs > 3600).any():\n",
    "        anomalies[\"timestamp_diff\"] = True\n",
    "    else:\n",
    "        anomalies[\"timestamp_diff\"] = False\n",
    "\n",
    "    # Check for missing values in X, Y, Speed\n",
    "    for col in ['X', 'Y', 'Speed']:  # Assuming these columns exist\n",
    "        if col in seg_df.columns:\n",
    "            if seg_df[col].isnull().any():\n",
    "                anomalies[f\"{col}_na\"] = True\n",
    "            else:\n",
    "                anomalies[f\"{col}_na\"] = False\n",
    "\n",
    "    # Statistical checks, flag rows that exceed mean +/- 3*std\n",
    "    for col in ['X', 'Y', 'Speed']:\n",
    "        if col in seg_df.columns:\n",
    "            mean = seg_df[col].mean()\n",
    "            std = seg_df[col].std()\n",
    "            if ((seg_df[col] > mean + 3 * std) | (seg_df[col] < mean - 3 * std)).any():\n",
    "                anomalies[f\"{col}_stat_anomaly\"] = True\n",
    "            else:\n",
    "                anomalies[f\"{col}_stat_anomaly\"] = False\n",
    "        \n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a matrix where each line represents a worm and each column represents a segment. Each cell contains a dictionary of anomalies found in that segment.\n",
    "# All worms don't have the same number of segments so we can fill missing segments with None\n",
    "\n",
    "anomaly_matrix = {}\n",
    "for treatment in [CONTROL, TREATED]:\n",
    "    treatment_dir = os.path.join(PREPROCESSED_DIR, treatment)\n",
    "    for file_name in os.listdir(treatment_dir):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(treatment_dir, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "            max_segment = df['Segment'].max()\n",
    "            worm_anomalies = {}\n",
    "            for segment in range(max_segment + 1):\n",
    "                anomalies = sanity_check_segment(file_path, segment)\n",
    "                worm_anomalies[segment] = anomalies\n",
    "            anomaly_matrix[file_name] = worm_anomalies\n",
    "\n",
    "# Save the results in an excel file\n",
    "anomaly_df = pd.DataFrame.from_dict({(i,j): anomaly_matrix[i][j] \n",
    "                           for i in anomaly_matrix.keys() \n",
    "                           for j in anomaly_matrix[i].keys()}, orient='index')\n",
    "anomaly_df.to_excel('segment_anomalies.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6be99947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that the first row of each csv is missing speed value, so we can assume it to 0\n",
    "# We can also scale the coordinates to be between 0 and 1 based on the min and max values of X and Y in each file\n",
    "# We can also clip the speed that could be weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d2fe975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are some nan values in the classifier csv files, print from where columns do they come\n",
    "# Print also the number of nan values per file\n",
    "classifier_dir = 'preprocessed_data_for_classifier'\n",
    "for file in os.listdir(classifier_dir):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(classifier_dir, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        nan_counts = df.isna().sum()\n",
    "        total_nans = nan_counts.sum()\n",
    "        if total_nans > 0:\n",
    "            print(f\"File: {file}\")\n",
    "            print(f\"Total NaN values: {total_nans}\")\n",
    "            print(\"NaN counts per column:\")\n",
    "            print(nan_counts[nan_counts > 0])\n",
    "            print(\"\\n\") \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
